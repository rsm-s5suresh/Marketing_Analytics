{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Multinomial Logit Model\"\n",
        "author: \"Shruthi Suresh\"\n",
        "date: today\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "This assignment expores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm. \n",
        "\n",
        "\n",
        "## 1. Likelihood for the Multi-nomial Logit (MNL) Model\n",
        "\n",
        "Suppose we have $i=1,\\ldots,n$ consumers who each select exactly one product $j$ from a set of $J$ products. The outcome variable is the identity of the product chosen $y_i \\in \\{1, \\ldots, J\\}$ or equivalently a vector of $J-1$ zeros and $1$ one, where the $1$ indicates the selected product. For example, if the third product was chosen out of 3 products, then either $y=3$ or $y=(0,0,1)$ depending on how we want to represent it. Suppose also that we have a vector of data on each product $x_j$ (eg, brand, price, etc.). \n",
        "\n",
        "We model the consumer's decision as the selection of the product that provides the most utility, and we'll specify the utility function as a linear function of the product characteristics:\n",
        "\n",
        "$$ U_{ij} = x_j'\\beta + \\epsilon_{ij} $$\n",
        "\n",
        "where $\\epsilon_{ij}$ is an i.i.d. extreme value error term. \n",
        "\n",
        "The choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer $i$ chooses product $j$:\n",
        "\n",
        "$$ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} $$\n",
        "\n",
        "For example, if there are 3 products, the probability that consumer $i$ chooses product 3 is:\n",
        "\n",
        "$$ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} $$\n",
        "\n",
        "A clever way to write the individual likelihood function for consumer $i$ is the product of the $J$ probabilities, each raised to the power of an indicator variable ($\\delta_{ij}$) that indicates the chosen product:\n",
        "\n",
        "$$ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}$$\n",
        "\n",
        "Notice that if the consumer selected product $j=3$, then $\\delta_{i3}=1$ while $\\delta_{i1}=\\delta_{i2}=0$ and the likelihood is:\n",
        "\n",
        "$$ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} $$\n",
        "\n",
        "The joint likelihood (across all consumers) is the product of the $n$ individual likelihoods:\n",
        "\n",
        "$$ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} $$\n",
        "\n",
        "And the joint log-likelihood function is:\n",
        "\n",
        "$$ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) $$\n",
        "\n",
        "\n",
        "\n",
        "## 2. Simulate Conjoint Data\n",
        "\n",
        "We will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a \"no choice\" option; each simulated respondent must select one of the 3 alternatives. \n",
        "\n",
        "Each alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from \\$4 to \\$32 in increments of \\$4.\n",
        "\n",
        "The part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer $i$ for hypothethical streaming service $j$ is \n",
        "\n",
        "$$\n",
        "u_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n",
        "$$\n",
        "\n",
        "where the variables are binary indicators and $\\varepsilon$ is Type 1 Extreme Value (ie, Gumble) distributed.\n",
        "\n",
        "The following code provides the simulation of the conjoint data.\n",
        "\n",
        ":::: {.callout-note collapse=\"true\"}"
      ],
      "id": "681ec181"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from itertools import product\n",
        "\n",
        "# Set seed for reproducibility\n",
        "np.random.seed(123)\n",
        "\n",
        "# Define attributes\n",
        "brands = ['N', 'P', 'H']  # Netflix, Prime, Hulu\n",
        "ads = ['Yes', 'No']\n",
        "prices = np.arange(8, 33, 4)  # 8 to 32, step 4\n",
        "\n",
        "# Generate all possible profiles\n",
        "profiles = pd.DataFrame(list(product(brands, ads, prices)), columns=[\"brand\", \"ad\", \"price\"])\n",
        "m = len(profiles)\n",
        "\n",
        "# Define part-worth utilities (true coefficients)\n",
        "b_util = {\"N\": 1.0, \"P\": 0.5, \"H\": 0.0}\n",
        "a_util = {\"Yes\": -0.8, \"No\": 0.0}\n",
        "p_util = lambda p: -0.1 * p\n",
        "\n",
        "# Simulation parameters\n",
        "n_peeps = 100\n",
        "n_tasks = 10\n",
        "n_alts = 3\n",
        "\n",
        "# Simulate one respondent’s data\n",
        "def sim_one(id):\n",
        "    datalist = []\n",
        "    for t in range(1, n_tasks + 1):\n",
        "        sampled = profiles.sample(n=n_alts).copy()\n",
        "        sampled[\"resp\"] = id\n",
        "        sampled[\"task\"] = t\n",
        "        sampled[\"v\"] = sampled.apply(\n",
        "            lambda row: b_util[row[\"brand\"]] + a_util[row[\"ad\"]] + p_util(row[\"price\"]),\n",
        "            axis=1\n",
        "        )\n",
        "        sampled[\"e\"] = -np.log(-np.log(np.random.rand(n_alts)))  # Gumbel noise\n",
        "        sampled[\"u\"] = sampled[\"v\"] + sampled[\"e\"]\n",
        "        sampled[\"choice\"] = (sampled[\"u\"] == sampled[\"u\"].max()).astype(int)\n",
        "        datalist.append(sampled)\n",
        "    return pd.concat(datalist, ignore_index=True)\n",
        "\n",
        "# Simulate data for all respondents\n",
        "conjoint_data = pd.concat([sim_one(i) for i in range(1, n_peeps + 1)], ignore_index=True)\n",
        "\n",
        "# Keep only observable columns\n",
        "conjoint_data = conjoint_data[[\"resp\", \"task\", \"brand\", \"ad\", \"price\", \"choice\"]]\n",
        "\n",
        "# View sample\n",
        "conjoint_data.head()"
      ],
      "id": "35fb9d08",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::::\n",
        "\n",
        "\n",
        "\n",
        "## 3. Preparing the Data for Estimation\n",
        "\n",
        "The \"hard part\" of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer $i$, covariate $k$, and product $j$) instead of the typical 2 dimensions for cross-sectional regression models (consumer $i$ and covariate $k$). The fact that each task for each respondent has the same number of alternatives (3) helps.  In addition, we need to convert the categorical variables for brand and ads into binary variables.\n"
      ],
      "id": "68ab48e8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "conjoint_data = pd.read_csv(\"conjoint_data.csv\")\n",
        "print(\"Columns in dataset:\", conjoint_data.columns.tolist())  # Confirm 'resp' and 'task' are present\n",
        "\n",
        "# Step 1: One-hot encode categorical variables (drop first level to avoid multicollinearity)\n",
        "X_df = pd.get_dummies(conjoint_data[[\"brand\", \"ad\"]], drop_first=True)\n",
        "\n",
        "# Step 2: Add numeric price\n",
        "X_df[\"price\"] = conjoint_data[\"price\"]\n",
        "\n",
        "# Step 3: Add the binary response variable\n",
        "X_df[\"choice\"] = conjoint_data[\"choice\"]\n",
        "\n",
        "# Step 4: Create design matrix and target\n",
        "X = X_df.drop(columns=[\"choice\"])\n",
        "y = X_df[\"choice\"]\n",
        "\n",
        "# Optional: Combine everything for easy preview\n",
        "# Convert X and y back to DataFrame with correct column names\n",
        "Xy_df = pd.concat([X.reset_index(drop=True), y.reset_index(drop=True)], axis=1)\n",
        "\n",
        "# Add respondent and task info\n",
        "conjoint_ready = pd.concat(\n",
        "    [conjoint_data[[\"resp\", \"task\"]].reset_index(drop=True), Xy_df],\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# Step 5: Display the final prepared DataFrame\n",
        "print(conjoint_ready.head())\n"
      ],
      "id": "fdf89d4d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Estimation via Maximum Likelihood\n"
      ],
      "id": "1451b76d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# -------------------------------------------\n",
        "# Step 0: Imports (run this after restarting kernel)\n",
        "# -------------------------------------------\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.optimize import minimize\n",
        "from scipy.stats import norm\n",
        "\n",
        "# -------------------------------------------\n",
        "# Step 1: Extract Design Matrix X and Target y\n",
        "# -------------------------------------------\n",
        "# Assumes you already have `conjoint_ready` from previous steps\n",
        "X = conjoint_ready[[\"brand_N\", \"brand_P\", \"ad_Yes\", \"price\"]].values\n",
        "y = conjoint_ready[\"choice\"].values\n",
        "alt_per_task = 3  # 3 alternatives per choice task\n",
        "\n",
        "# Convert to float64 to ensure proper numerical operations\n",
        "X = X.astype(np.float64)\n",
        "y = y.astype(np.int32)\n",
        "\n",
        "print(f\"X shape: {X.shape}\")\n",
        "print(f\"y shape: {y.shape}\")\n",
        "print(f\"X dtype: {X.dtype}\")\n",
        "print(f\"y dtype: {y.dtype}\")\n",
        "\n",
        "# -------------------------------------------\n",
        "# Step 2: Define Log-Likelihood Function\n",
        "# -------------------------------------------\n",
        "def mnl_log_likelihood(beta, X, y, alt_per_task=3):\n",
        "    \"\"\"\n",
        "    Compute the negative log-likelihood of the multinomial logit model.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        n_obs = X.shape[0]\n",
        "        n_tasks = n_obs // alt_per_task\n",
        "        \n",
        "        # Ensure beta is float64\n",
        "        beta = np.array(beta, dtype=np.float64)\n",
        "        \n",
        "        # Calculate utilities\n",
        "        X_beta = np.dot(X, beta)\n",
        "        X_beta = X_beta.reshape((n_tasks, alt_per_task))\n",
        "        y_reshaped = y.reshape((n_tasks, alt_per_task))\n",
        "        \n",
        "        # Softmax with numerical stability\n",
        "        max_Xb = np.max(X_beta, axis=1, keepdims=True)\n",
        "        exp_terms = np.exp(X_beta - max_Xb)\n",
        "        log_sum_exp = np.log(np.sum(exp_terms, axis=1, keepdims=True))\n",
        "        log_probs = X_beta - max_Xb - log_sum_exp\n",
        "        \n",
        "        # Calculate negative log-likelihood\n",
        "        chosen_log_probs = log_probs[y_reshaped == 1]\n",
        "        neg_log_likelihood = -np.sum(chosen_log_probs)\n",
        "        \n",
        "        # Check for invalid values\n",
        "        if not np.isfinite(neg_log_likelihood):\n",
        "            return 1e10  # Return large value if optimization goes wrong\n",
        "            \n",
        "        return neg_log_likelihood\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error in log-likelihood function: {e}\")\n",
        "        return 1e10\n",
        "\n",
        "# -------------------------------------------\n",
        "# Step 3: Test the function first\n",
        "# -------------------------------------------\n",
        "initial_beta = np.zeros(X.shape[1], dtype=np.float64)\n",
        "print(f\"Initial beta: {initial_beta}\")\n",
        "\n",
        "# Test the log-likelihood function\n",
        "test_ll = mnl_log_likelihood(initial_beta, X, y, alt_per_task)\n",
        "print(f\"Initial log-likelihood: {test_ll}\")\n",
        "\n",
        "# -------------------------------------------\n",
        "# Step 4: Estimate Using scipy.optimize.minimize\n",
        "# -------------------------------------------\n",
        "print(\"Starting optimization...\")\n",
        "result = minimize(\n",
        "    mnl_log_likelihood, \n",
        "    initial_beta, \n",
        "    args=(X, y, alt_per_task), \n",
        "    method='BFGS',\n",
        "    options={'disp': True, 'maxiter': 1000}\n",
        ")\n",
        "\n",
        "print(f\"Optimization successful: {result.success}\")\n",
        "print(f\"Message: {result.message}\")\n",
        "\n",
        "# -------------------------------------------\n",
        "# Step 5: Extract Results and Standard Errors\n",
        "# -------------------------------------------\n",
        "if result.success:\n",
        "    beta_hat = result.x\n",
        "    \n",
        "    # Check if hessian inverse is available\n",
        "    if hasattr(result, 'hess_inv') and result.hess_inv is not None:\n",
        "        if isinstance(result.hess_inv, np.ndarray):\n",
        "            hessian_inv = result.hess_inv\n",
        "        else:\n",
        "            # Sometimes hess_inv is a LinearOperator, convert to array\n",
        "            hessian_inv = result.hess_inv.todense() if hasattr(result.hess_inv, 'todense') else np.array(result.hess_inv)\n",
        "        \n",
        "        std_errors = np.sqrt(np.diag(hessian_inv))\n",
        "        \n",
        "        # -------------------------------------------\n",
        "        # Step 6: Confidence Intervals (95%)\n",
        "        # -------------------------------------------\n",
        "        z = norm.ppf(0.975)\n",
        "        ci_lower = beta_hat - z * std_errors\n",
        "        ci_upper = beta_hat + z * std_errors\n",
        "        \n",
        "        # -------------------------------------------\n",
        "        # Step 7: Tabulate Results\n",
        "        # -------------------------------------------\n",
        "        param_names = [\"beta_netflix\", \"beta_prime\", \"beta_ads\", \"beta_price\"]\n",
        "        mle_results = pd.DataFrame({\n",
        "            \"Parameter\": param_names,\n",
        "            \"Estimate\": beta_hat,\n",
        "            \"Std_Error\": std_errors,\n",
        "            \"95% CI Lower\": ci_lower,\n",
        "            \"95% CI Upper\": ci_upper\n",
        "        })\n",
        "\n",
        "        # Save MLE results to CSV\n",
        "        mle_results.to_csv(\"mle_results.csv\", index=False)\n",
        "\n",
        "        \n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"MAXIMUM LIKELIHOOD ESTIMATION RESULTS\")\n",
        "        print(\"=\"*50)\n",
        "        print(mle_results.round(4))\n",
        "        \n",
        "    else:\n",
        "        print(\"Warning: Hessian inverse not available. Results without standard errors:\")\n",
        "        param_names = [\"beta_netflix\", \"beta_prime\", \"beta_ads\", \"beta_price\"]\n",
        "        basic_results = pd.DataFrame({\n",
        "            \"Parameter\": param_names,\n",
        "            \"Estimate\": beta_hat\n",
        "        })\n",
        "        print(basic_results.round(4))\n",
        "        \n",
        "else:\n",
        "    print(\"Optimization failed!\")\n",
        "    print(f\"Message: {result.message}\")\n"
      ],
      "id": "355b4011",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Estimation via Bayesian Methods\n"
      ],
      "id": "ed5b9dcc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# -------------------------------------------\n",
        "# Step 1: Define log-prior function\n",
        "# -------------------------------------------\n",
        "def log_prior(beta):\n",
        "    # Priors: N(0, 5^2) for binary variables, N(0, 1^2) for price\n",
        "    binary_priors = -0.5 * (beta[:3] / 5)**2 - 3 * np.log(5 * np.sqrt(2 * np.pi))\n",
        "    price_prior = -0.5 * (beta[3] / 1)**2 - np.log(np.sqrt(2 * np.pi))\n",
        "    return binary_priors.sum() + price_prior\n",
        "\n",
        "# -------------------------------------------\n",
        "# Step 2: Define log-posterior (log-likelihood + log-prior)\n",
        "# -------------------------------------------\n",
        "def log_posterior(beta, X, y, alt_per_task):\n",
        "    loglik = -mnl_log_likelihood(beta, X, y, alt_per_task)  # log-likelihood is negative\n",
        "    logpri = log_prior(beta)\n",
        "    return loglik + logpri\n",
        "\n",
        "\n",
        "# -------------------------------------------\n",
        "# Step 3: Metropolis-Hastings Sampler\n",
        "# -------------------------------------------\n",
        "n_iter = 11000\n",
        "burn_in = 1000\n",
        "n_params = X.shape[1]\n",
        "\n",
        "samples = np.zeros((n_iter, n_params))\n",
        "current_beta = np.zeros(n_params)\n",
        "current_logpost = log_posterior(current_beta, X, y, alt_per_task)\n",
        "\n",
        "# Proposal SDs: binary params N(0, 0.05), price param N(0, 0.005)\n",
        "proposal_sds = np.array([0.05, 0.05, 0.05, 0.005])\n",
        "\n",
        "accept_count = 0\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "for i in range(n_iter):\n",
        "    proposal = current_beta + np.random.normal(0, proposal_sds)\n",
        "    proposal_logpost = log_posterior(proposal, X, y, alt_per_task)\n",
        "\n",
        "    log_accept_ratio = proposal_logpost - current_logpost\n",
        "\n",
        "    if np.log(np.random.rand()) < log_accept_ratio:\n",
        "        current_beta = proposal\n",
        "        current_logpost = proposal_logpost\n",
        "        accept_count += 1\n",
        "\n",
        "    samples[i, :] = current_beta\n",
        "\n",
        "accept_rate = accept_count / n_iter\n",
        "print(f\"Acceptance rate: {accept_rate:.3f}\")\n",
        "\n",
        "# -------------------------------------------\n",
        "# Step 4: Posterior Summary (after burn-in)\n",
        "# -------------------------------------------\n",
        "posterior_samples = samples[burn_in:, :]\n",
        "param_names = [\"beta_netflix\", \"beta_prime\", \"beta_ads\", \"beta_price\"]\n",
        "\n",
        "posterior_df = pd.DataFrame(posterior_samples, columns=param_names)\n",
        "\n",
        "posterior_summary = posterior_df.describe(percentiles=[0.025, 0.975]).T\n",
        "posterior_summary = posterior_summary[[\"mean\", \"std\", \"2.5%\", \"97.5%\"]]\n",
        "posterior_summary.columns = [\"Mean\", \"Std_Dev\", \"95% CI Lower\", \"95% CI Upper\"]\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"BAYESIAN POSTERIOR SUMMARY\")\n",
        "print(\"=\"*50)\n",
        "print(posterior_summary.round(4))\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Trace plot\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(posterior_df[\"beta_price\"])\n",
        "plt.title(\"Trace Plot: beta_price\")\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"Value\")\n",
        "\n",
        "# Histogram\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.histplot(posterior_df[\"beta_price\"], bins=30, kde=True)\n",
        "plt.title(\"Posterior Distribution: beta_price\")\n",
        "plt.xlabel(\"Value\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# -------------------------------------------\n",
        "# Debug the CSV loading issue\n",
        "# -------------------------------------------\n",
        "\n",
        "#file_path = \"C:\\\\Users\\\\Shruthi Suresh\\\\shruthis_website\\\\Projects\\\\Homework3\\\\mle_results.csv\"\n",
        "\n",
        "#mle_results = pd.read_csv(file_path)\n",
        "#print(\"Comparison: MLE vs Bayesian Estimates\")\n",
        "#combined = mle_results.set_index(\"Parameter\").join(posterior_summary, on=\"Parameter\")\n",
        "#print(combined.round(4))"
      ],
      "id": "52990ab0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Discussion\n",
        "\n",
        "\n",
        "### Interpreting Parameter Estimates :\n",
        "\n",
        "If we had not simulated the data and were working with real-world conjoint responses, we would interpret the estimated parameters as revealed preferences derived from observed consumer choices. The fact that the model provides statistically significant estimates (with narrow confidence or credible intervals) suggests that the attributes included—brand, presence of ads, and price—are meaningful drivers of choice behavior.\n",
        "\n",
        "- **$\\beta_\\text{Netflix} > \\beta_\\text{Prime}$**: This result implies that, on average, consumers prefer **Netflix** over **Amazon Prime**, all else being equal. Since **Hulu** is the reference category (omitted in the dummy encoding), this also suggests that **Netflix** is the most preferred brand among the three, followed by **Prime**, then **Hulu**.\n",
        "\n",
        "- **$\\beta_\\text{price} < 0$**: A negative coefficient on price is not only intuitive but essential for model validity. It indicates that, all else being equal, higher-priced options are **less likely** to be chosen. The magnitude tells us how **sensitive** consumers are to price changes.\n",
        "\n",
        "---\n",
        "\n",
        "### Simulating and Estimating a Multi-Level (Hierarchical) Model\n",
        "\n",
        "To move from a simple fixed-effects model to a **multi-level** (random-parameter or hierarchical) model, the key change is to allow **individual-level variation** in preferences. This means we no longer assume a single set of $\\beta$ coefficients for the entire population, but instead allow each respondent to have their own set of $\\beta_i$ values drawn from a population distribution.\n",
        "\n",
        "#### Key changes to simulate hierarchical data:\n",
        "\n",
        "- Instead of applying one common set of part-worth utilities, draw  \n",
        "  $\\beta_i \\sim \\mathcal{N}(\\mu, \\Sigma)$  \n",
        "  for each respondent $i$.\n",
        "\n",
        "- These $\\beta_i$ vectors are then used to compute utilities and simulate choices, allowing heterogeneity in behavior.\n",
        "\n",
        "#### Key changes for estimation:\n",
        "\n",
        "- Use **Bayesian hierarchical modeling** (e.g., via MCMC with group-level priors), or **Mixed Logit** models with simulated maximum likelihood.\n",
        "- Estimate the **distribution of preferences** (e.g., mean and covariance of $\\beta$ across individuals) instead of just point estimates.\n",
        "- Tools like **Stan**, **PyMC**, or hierarchical models in **scikit-learn** or **statsmodels** can be used.\n",
        "\n",
        "This hierarchical approach better captures the reality that not all consumers behave the same and allows us to tailor predictions or develop segment-specific strategies accordingly.\n"
      ],
      "id": "a57fe4fb"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "C:\\Users\\Shruthi Suresh\\AppData\\Local\\Programs\\Python\\Python313\\share\\jupyter\\kernels\\python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}