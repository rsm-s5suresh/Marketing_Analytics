{"title":"Poisson Regression Examples","markdown":{"yaml":{"title":"Poisson Regression Examples","author":"Shruthi Suresh","date":"today","callout-appearance":"minimal"},"headingText":"Blueprinty Case Study","containsRefs":false,"markdown":"\n\n\n\n### Introduction\n\nBlueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty's software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty's software and after using it. Unfortunately, such data is not available. \n\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm's number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty's software. The marketing team would like to use this data to make the claim that firms using Blueprinty's software are more successful in getting their patent applications approved.\n\ntodo: Read in data.\n\n### Data\n```{python}\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Read in the data\ndf = pd.read_csv(\"blueprinty.csv\")\n\n# Display basic info and preview\nprint(df.info())\nprint(df.head())\n```\n\ntodo: Compare histograms and means of number of patents by customer status. What do you observe?\n\n```{python}\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv(\"blueprinty.csv\")\n\n# Calculate mean number of patents by customer status\nmean_patents = df.groupby('iscustomer')['patents'].mean()\nprint(\"Mean number of patents:\\n\", mean_patents)\n\n# Plot histograms for customer vs non-customer\nplt.figure(figsize=(12, 6))\n\nplt.hist(df[df['iscustomer'] == 0]['patents'], bins=20, alpha=0.6, label='Non-Customers', color='red')\nplt.hist(df[df['iscustomer'] == 1]['patents'], bins=20, alpha=0.6, label='Customers', color='blue')\n\nplt.xlabel('Number of Patents')\nplt.ylabel('Number of Firms')\nplt.title('Patent Distribution by Blueprinty Usage')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n```\n### Histogram Analysis: Patent Distribution by Blueprinty Usage\n\n#### ðŸ”¹ Shift in Distribution\n- Customers have a **rightward shift** in their distribution compared to non-customers.\n- This suggests that **Blueprinty users tend to receive more patents**.\n\n#### ðŸ”¹ Higher Concentration at 4â€“8 Patents\n- The **blue bars dominate** in the range of **4 to 8 patents**, indicating a higher share of **high-performing firms** among Blueprinty users.\n\n#### ðŸ”¹ Non-Customers Clustered Lower\n- The **red bars are more concentrated** between **2 to 4 patents**, suggesting non-customers more frequently have **lower patent counts**.\n\n#### ðŸ”¹ Right-Tail Presence\n- A few firms with **10+ patents** appear in the distribution, primarily among Blueprinty users.\n- This may reflect a **subset of highly innovative firms** that benefit from using the software.\n\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\ntodo: Compare regions and ages by customer status. What do you observe?\n\n```{python}\n!pip install seaborn\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load the data\ndf = pd.read_csv(\"blueprinty.csv\")\n\n# Boxplot: Firm age by customer status\nplt.figure(figsize=(8, 5))\nsns.boxplot(x='iscustomer', y='age', data=df)\nplt.xticks([0, 1], ['Non-Customers', 'Customers'])\nplt.title('Firm Age by Blueprinty Customer Status')\nplt.xlabel('Blueprinty Customer')\nplt.ylabel('Firm Age')\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n# Cross-tabulation: Region by customer status (percentage within region)\nregion_counts = pd.crosstab(df['region'], df['iscustomer'], normalize='index') * 100\nregion_counts.columns = ['Non-Customers (%)', 'Customers (%)']\n\n# Sort by customer percentage and display\nregion_counts = region_counts.sort_values(by='Customers (%)', ascending=False)\nprint(\"\\nRegional Blueprinty Usage (%):\\n\")\nprint(region_counts.round(2))\n```\n### Systematic Differences in Customer vs. Non-Customer Firms\n\n1. **Age Distribution**:\n   - Blueprinty customers tend to be **slightly older** than non-customers, with a higher median firm age and more firms in the upper age range.\n\n2. **Regional Skew**:\n   - **Northeast** is the only region where a **majority of firms (54.6%) are customers**.\n   - All other regions (South, Southwest, Midwest, Northwest) have **customer rates below 20%**.\n\n3. **Customer Concentration**:\n   - Blueprinty adoption is **not uniform** across regions, with the **highest concentration of customers** in the **Northeast**.\n\n4. **Importance for Modeling**:\n   - Because age and region are **not randomly distributed** across customer groups, it's important to **control for both** when modeling patent outcomes to avoid biased estimates.\n\n\n### Estimation of Simple Poisson Model\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\n_todo: Write down mathematically the likelihood for_ $Y \\sim \\text{Poisson}(\\lambda)$. Note that $f(Y|\\lambda) = e^{-\\lambda}\\lambda^Y/Y!$.\n\n### Likelihood for Poisson Model\n\nWe model the number of patents \\($Y_i$\\) awarded to firm \\($i$\\) over 5 years as following a Poisson distribution:\n\n$$\nY_i \\sim \\text{Poisson}(\\lambda)\n$$\n\nThe probability mass function (PMF) for each observation is:\n\n$$\nf(Y_i | \\lambda) = \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n$$\n\nAssuming we have \\($n$\\) independent firms, the **joint likelihood function** is the product of all individual probabilities:\n\n$$\nL(\\lambda) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!} \n= e^{-n\\lambda} \\lambda^{\\sum_{i=1}^{n} Y_i} \\prod_{i=1}^{n} \\frac{1}{Y_i!}\n$$\n\n---\n\n### Log-Likelihood Function\n\nTo make the math easier for maximization, we take the logarithm of the likelihood function:\n\n$$\n\\ell(\\lambda) = \\log L(\\lambda) \n= -n\\lambda + \\left( \\sum_{i=1}^{n} Y_i \\right) \\log \\lambda - \\sum_{i=1}^{n} \\log(Y_i!)\n$$\n\nThis log-likelihood is what we will maximize to find the **Maximum Likelihood Estimate (MLE)** of ($\\lambda$\\)\n\n_todo: Code the likelihood (or log-likelihood) function for the Poisson model. This is a function of lambda and Y. For example:_\n\n```\npoisson_loglikelihood <- function(lambda, Y){\n   ...\n}\n```\n\n```{python}\nimport numpy as np\nfrom scipy.special import gammaln  # for log(Y!) using gammaln(Y+1)\n\ndef poisson_loglikelihood(lam, Y):\n    \"\"\"\n    Compute the log-likelihood of a Poisson model.\n    \n    Parameters:\n    - lam: float, the Poisson rate parameter Î»\n    - Y: array-like, observed count data (e.g., number of patents)\n    \n    Returns:\n    - log_likelihood: float, the total log-likelihood given Î» and Y\n    \"\"\"\n    Y = np.array(Y)\n    log_likelihood = -lam * len(Y) + np.sum(Y * np.log(lam) - gammaln(Y + 1))\n    return log_likelihood\n```\n\ntodo: Use your function to plot lambda on the horizontal axis and the likelihood (or log-likelihood) on the vertical axis for a range of lambdas (use the observed number of patents as the input for Y).\n\n```{python}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.special import gammaln\n\n# Load the dataset\ndf = pd.read_csv(\"blueprinty.csv\")\n\n# Define the Poisson log-likelihood function\ndef poisson_loglikelihood(lam, Y):\n    Y = np.array(Y)\n    return -lam * len(Y) + np.sum(Y * np.log(lam) - gammaln(Y + 1))\n\n# Use actual patent data from the dataset\nY_observed = df['patents'].values\n\n# Generate a range of lambda values to evaluate\nlambda_range = np.linspace(0.1, 10, 200)  # Avoid lambda = 0 to prevent log(0)\nlog_likelihoods = [poisson_loglikelihood(lam, Y_observed) for lam in lambda_range]\n\n# Plot the log-likelihood curve\nplt.figure(figsize=(10, 5))\nplt.plot(lambda_range, log_likelihoods, color='purple')\nplt.title(\"Poisson Log-Likelihood vs Lambda\")\nplt.xlabel(\"Lambda (Î»)\")\nplt.ylabel(\"Log-Likelihood\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n```\n\n\n```{python}\nimport pandas as pd\n\n# Load the dataset\ndf = pd.read_csv(\"blueprinty.csv\")\n\n# Calculate the MLE of lambda (mean of observed patent counts)\nlambda_mle = df['patents'].mean()\n\n# Print the result\nprint(f\"MLE of lambda (Î»Ì‚): {lambda_mle:.4f}\")\n```\n\n_todo: Find the MLE by optimizing your likelihood function with optim() in R or sp.optimize() in Python._  do it in python\n```{python}\nimport pandas as pd\nimport numpy as np\nfrom scipy.special import gammaln\nfrom scipy.optimize import minimize_scalar\n\n# Load the dataset\ndf = pd.read_csv(\"blueprinty.csv\")\nY_observed = df['patents'].values\n\n# Define the Poisson log-likelihood function\ndef poisson_loglikelihood(lam, Y):\n    Y = np.array(Y)\n    return -lam * len(Y) + np.sum(Y * np.log(lam) - gammaln(Y + 1))\n\n# Negative log-likelihood (since optimizers minimize by default)\ndef neg_poisson_loglikelihood(lam, Y):\n    return -poisson_loglikelihood(lam, Y)\n\n# Use scipy.optimize to find the lambda that minimizes the negative log-likelihood\nresult = minimize_scalar(neg_poisson_loglikelihood, bounds=(0.1, 10), args=(Y_observed,), method='bounded')\n\n# Extract the MLE of lambda\nlambda_mle = result.x\n\n# Print the result\nprint(f\"MLE of lambda (Î»Ì‚) from optimization: {lambda_mle:.4f}\")\n```\n\n\n### Estimation of Poisson Regression Model\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that $Y_i = \\text{Poisson}(\\lambda_i)$ where $\\lambda_i = \\exp(X_i'\\beta)$. The interpretation is that the success rate of patent awards is not constant across all firms ($\\lambda$) but rather is a function of firm characteristics $X_i$. Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n_todo: Update your likelihood or log-likelihood function with an additional argument to take in a covariate matrix X. Also change the parameter of the model from lambda to the beta vector. In this model, lambda must be a positive number, so we choose the inverse link function g_inv() to be exp() so that_ $\\lambda_i = e^{X_i'\\beta}$. _For example:_\n\npoisson_regression_likelihood <- function(beta, Y, X){\n   ...\n}\n\n```{python}\nimport numpy as np\nfrom scipy.special import gammaln        # stable log(y!)\n\ndef poisson_regression_loglik(beta, y, X):\n    \"\"\"\n    Log-likelihood for a Poisson GLM with log link.\n\n    Parameters\n    ----------\n    beta : array-like, shape (p,)\n        Coefficient vector (includes intercept if X has a 1s column).\n    y : array-like, shape (n,)\n        Observed non-negative counts.\n    X : array-like, shape (n, p)\n        Covariate matrix.\n\n    Returns\n    -------\n    float\n        â„“(Î²) = Î£ [ y_iÂ·(X_i Î²)  âˆ’  exp(X_i Î²)  âˆ’  log(y_i!) ].\n    \"\"\"\n    beta = np.asarray(beta, dtype=float)\n    y    = np.asarray(y,    dtype=float)\n\n    eta  = X @ beta            # linear predictor  (n,)\n    lam  = np.exp(eta)         # inverse-link â‡’ Î»_i > 0\n\n    return (y * eta  -  lam  -  gammaln(y + 1)).sum()\n```\n\n_todo: Use your function along with R's optim() or Python's sp.optimize() to find the MLE vector and the Hessian of the Poisson model with covariates. Specifically, the first column of X should be all 1's to enable a constant term in the model, and the subsequent columns should be age, age squared, binary variables for all but one of the regions, and the binary customer variable. Use the Hessian to find standard errors of the beta parameter estimates and present a table of coefficients and standard errors._\n\n```{python}\n!pip install scikit-learn\n!pip install scipy\nimport pandas as pd\nimport numpy as np\nfrom scipy.special import gammaln\nfrom scipy.optimize import minimize\nfrom numpy.linalg import inv\nfrom sklearn.preprocessing import StandardScaler\n\nimport pandas as pd\nimport statsmodels.api as sm          # convenient optimiser + Hessian\n\nblueprinty = pd.read_csv(\"blueprinty.csv\")\n\nX = pd.DataFrame({\n    \"const\"     : 1,                                     # intercept\n    \"age\"       : blueprinty[\"age\"],\n    \"age_sq\"    : blueprinty[\"age\"]**2,\n    \"region_NE\" : (blueprinty[\"region\"]==\"Northeast\").astype(int),\n    \"region_NW\" : (blueprinty[\"region\"]==\"Northwest\").astype(int),\n    \"region_S\"  : (blueprinty[\"region\"]==\"South\").astype(int),\n    \"region_SW\" : (blueprinty[\"region\"]==\"Southwest\").astype(int),\n    \"customer\"  : blueprinty[\"iscustomer\"]\n})\ny = blueprinty[\"patents\"]\n\n# â”€â”€ Poisson GLM (log link) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nmodel = sm.GLM(y, X, family=sm.families.Poisson())\nres   = model.fit()                      # uses IRLS â‡’ MLE, Hessian\n\nresults = pd.DataFrame({\n    \"Coefficient\" : res.params,\n    \"Std. Error\"  : res.bse\n})\nprint(\"Poisson Regression Results\", results.round(4))\n```\n\n_todo: Check your results using R's glm() function or Python sm.GLM() function._\n\n```{python}\nimport numpy as np, pandas as pd, statsmodels.api as sm\nfrom scipy.special import gammaln\nfrom scipy.optimize import minimize\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n\nXm = X.values\n\n# â”€â”€ Custom log-likelihood & optimiser â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ndef pll(beta, y, X):\n    eta = X @ beta\n    lam = np.exp(eta)\n    return (y*eta - lam - gammaln(y + 1)).sum()\n\ndef neg_pll(beta, y, X):\n    return -pll(beta, y, X)\n\nbeta0    = np.zeros(Xm.shape[1])\nopt_res  = minimize(neg_pll, beta0, args=(y, Xm), method=\"BFGS\")\nbeta_hat = opt_res.x                     # â‡  custom MLE vector\n\n# â”€â”€ Built-in GLM (IRLS) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nglm_res = sm.GLM(y, X, family=sm.families.Poisson()).fit()\n\n# â”€â”€ Side-by-side comparison â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ncompare = pd.DataFrame({\n    \"Custom Î²Ì‚\": beta_hat,\n    \"GLM Î²Ì‚\"   : glm_res.params,\n    \"|Î”|\"      : np.abs(beta_hat - glm_res.params)\n}, index=X.columns).round(6)\n\ndisplay(compare)\n```\n\n_todo: Interpret the results._ \n\n## ðŸ“Š Interpretation of Poisson Regression Results\n\nEach coefficient represents the **log change** in the expected number of patents for a **1-unit change** in the predictor, holding other variables constant. Since a Poisson model uses a **log link**, we interpret changes in **multiplicative (percentage) terms** using `exp(coef)`.\n\n---\n\n### ðŸ”¹ `const` (Intercept): -0.5089\n- This is the baseline log expected number of patents when all other variables are zero.\n- Not directly meaningful but needed for model completeness.\n\n---\n\n### ðŸ”¹ `age`: 0.1486  \n- A 1-year increase in firm age is associated with an **increase of exp(0.1486) â‰ˆ 1.16 times** more expected patents (~16% increase), holding all else constant.\n\n---\n\n### ðŸ”¹ `age_sq`: -0.0030  \n- Indicates a **non-linear effect** of age: as firms get older, the rate of patenting eventually slows down.\n- Suggests a concave (inverted U-shaped) relationship between age and patent output.\n\n---\n\n### ðŸ”¹ Region Dummies (`region_NE`, `region_NW`, `region_S`, `region_SW`)\n- All are compared to the **reference category (likely \"Midwest\")**.\n- Coefficients are small, implying **minor regional differences** in patenting rates.\n- For example:\n  - `region_S = 0.0566` â†’ firms in the South have ~6% higher expected patent counts than Midwest.\n\n---\n\n### ðŸ”¹ `customer`: 0.2076  \n- Being a Blueprinty customer increases the expected number of patents by **exp(0.2076) â‰ˆ 1.23**.\n- Thatâ€™s a **23% increase in patenting rate**, all else equal â€” which supports the marketing teamâ€™s claim.\n\n---\n\n### âœ… Summary Insight:\n- **Age has a significant positive effect**, but with diminishing returns.\n- **Using Blueprinty software is strongly associated with more patents**.\n- **Regional effects are minor**.\n\n\n_todo: What do you conclude about the effect of Blueprinty's software on patent success? Because the beta coefficients are not directly interpretable, it may help to create two fake datasets: X_0 and X_1 where X_0 is the X data but with iscustomer=0 for every observation and X_1 is the X data but with iscustomer=1 for every observation. Then, use X_0 and your fitted model to get the vector of predicted number of patents (y_pred_0) for every firm in the dataset, and use X_1 to get Y_pred_1 for every firm. Then subtract y_pred_1 minus y_pred_0 and take the average of that vector of differences._\n\n\n## Quantifying Blueprintyâ€™s Impact via Counterfactual Prediction\n\nGoal:\nTo understand the impact of Blueprinty's software on patent success,\nwe use a fitted Poisson regression model to simulate two scenarios:\n1. All firms are assumed to NOT use Blueprinty (iscustomer = 0)\n2. All firms are assumed to use Blueprinty (iscustomer = 1)\n\nWe then:\n- Predict patent counts for each firm under both scenarios\n- Calculate the average increase in patent count (y_pred_1 - y_pred_0)\n- Calculate the relative percentage increase due to Blueprinty\n\n```{python}\n# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nfrom scipy.special import gammaln\n\n# Load the dataset\ndf = pd.read_csv(\"blueprinty.csv\")\n\n# Create the design matrix X and response variable y\nX = pd.DataFrame({\n    \"const\"     : 1,\n    \"age\"       : df[\"age\"],\n    \"age_sq\"    : df[\"age\"]**2,\n    \"region_NE\" : (df[\"region\"] == \"Northeast\").astype(int),\n    \"region_NW\" : (df[\"region\"] == \"Northwest\").astype(int),\n    \"region_S\"  : (df[\"region\"] == \"South\").astype(int),\n    \"region_SW\" : (df[\"region\"] == \"Southwest\").astype(int),\n    \"customer\"  : df[\"iscustomer\"]\n})\ny = df[\"patents\"]\n\n# Fit Poisson GLM\nglm_model = sm.GLM(y, X, family=sm.families.Poisson()).fit()\n\n# ---- Counterfactual Predictions ----\n\n# Scenario 1: All firms are non-customers (iscustomer = 0)\nX_0 = X.copy()\nX_0[\"customer\"] = 0\ny_pred_0 = glm_model.predict(X_0)\n\n# Scenario 2: All firms are customers (iscustomer = 1)\nX_1 = X.copy()\nX_1[\"customer\"] = 1\ny_pred_1 = glm_model.predict(X_1)\n\n# Compute average treatment effect and percent lift\navg_diff = (y_pred_1 - y_pred_0).mean()\npct_increase = avg_diff / y_pred_0.mean()\n\n# ---- Output Results ----\nprint(\"ðŸ“Š Counterfactual Analysis of Blueprinty Impact\")\nprint(f\"Average increase in patents per firm: {avg_diff:.3f}\")\nprint(f\"Relative lift from Blueprinty usage: {pct_increase:.1%}\")\n```\n\n## âœ… Interpretation & Conclusion: Blueprinty's Impact on Patent Success\n\n### Interpretation\n\n- When assuming all firms are **non-customers** (`iscustomer = 0`), we predict their expected number of patents using the Poisson model.\n- When assuming all firms are **Blueprinty customers** (`iscustomer = 1`), the predicted patent counts increase.\n- The **average increase** in expected patent counts is: 0.793 additional patents per firm over 5 years\n- This translates to a **relative lift** of: 23.1% increase in patent output.\n\n\n- This effect holds after **controlling for other factors** such as firm age and regional location.\n\n---\n\n### Conclusion\n\n- Blueprinty's software is associated with a **significant and positive effect** on patent productivity.\n- On average, firms using Blueprinty are expected to receive **nearly one extra patent** over five years.\n- The **23.1% lift** is substantial, providing strong evidence to support Blueprintyâ€™s value proposition.\n- These results are statistically credible and align with intuitive expectations, reinforcing the case for adopting Blueprinty among engineering firms.\n\n## AirBnB Case Study\n\n### Introduction\n\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City.  The data include the following variables:\n\n:::: {.callout-note collapse=\"true\"}\n### Variable Definitions\n\n    - `id` = unique ID number for each unit\n    - `last_scraped` = date when information scraped\n    - `host_since` = date when host first listed the unit on Airbnb\n    - `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n    - `room_type` = Entire home/apt., Private room, or Shared room\n    - `bathrooms` = number of bathrooms\n    - `bedrooms` = number of bedrooms\n    - `price` = price per night (dollars)\n    - `number_of_reviews` = number of reviews for the unit on Airbnb\n    - `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n    - `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n    - `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n    - `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n::::\n\n\n_todo: Assume the number of reviews is a good proxy for the number of bookings. Perform some exploratory data analysis to get a feel for the data, handle or drop observations with missing values on relevant variables, build one or more models (e.g., a poisson regression model for the number of bookings as proxied by the number of reviews), and interpret model coefficients to describe variation in the number of reviews as a function of the variables provided._\n\n```{python}\n# ðŸ“Š Full Exploratory Data Analysis for Airbnb NYC Listings\n\n# Step 1: Import necessary libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\n\n# Step 2: Load the dataset\ndf = pd.read_csv(\"airbnb.csv\")  # Adjust path if needed\n\n# Step 3: Keep relevant columns\ncolumns_to_use = [\n    \"number_of_reviews\", \"room_type\", \"bathrooms\", \"bedrooms\", \"price\",\n    \"review_scores_cleanliness\", \"review_scores_location\",\n    \"review_scores_value\", \"instant_bookable\"\n]\ndf = df[columns_to_use].copy()\n\n# Step 5: Convert types\ndf[\"instant_bookable\"] = (df[\"instant_bookable\"] == \"t\").astype(int)\nnumeric_cols = [\n    \"number_of_reviews\", \"bathrooms\", \"bedrooms\", \"price\",\n    \"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_value\"\n]\ndf[numeric_cols] = df[numeric_cols].apply(pd.to_numeric, errors='coerce')\n\n# Step 6: Summary Statistics\nsummary_stats = df.describe()\nprint(\"ðŸ“‹ Summary Statistics:\")\nprint(summary_stats.round(2))\n\n# Step 7: Histogram of number_of_reviews\nplt.figure(figsize=(10, 5))\nsns.histplot(df[\"number_of_reviews\"], bins=50, kde=False)\nplt.title(\"Distribution of Number of Reviews\")\nplt.xlabel(\"Number of Reviews\")\nplt.ylabel(\"Count\")\nplt.xlim(0, 200)\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n# Step 8: Boxplot - Number of Reviews by Room Type\nplt.figure(figsize=(8, 5))\nsns.boxplot(data=df, x=\"room_type\", y=\"number_of_reviews\")\nplt.title(\"Number of Reviews by Room Type\")\nplt.xlabel(\"Room Type\")\nplt.ylabel(\"Number of Reviews\")\nplt.ylim(0, 200)\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n# Step 9: Correlation Heatmap\nplt.figure(figsize=(10, 8))\ncorr = df[numeric_cols + [\"instant_bookable\"]].corr()\nsns.heatmap(corr, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\nplt.title(\"Correlation Heatmap of Numeric Features\")\nplt.tight_layout()\nplt.show()\n\n# Step 10: Scatterplot - Price vs Number of Reviews\nplt.figure(figsize=(10, 5))\nsns.scatterplot(data=df, x=\"price\", y=\"number_of_reviews\", alpha=0.5)\nplt.title(\"Price vs. Number of Reviews\")\nplt.xlabel(\"Price per Night ($)\")\nplt.ylabel(\"Number of Reviews\")\nplt.xlim(0, 1000)\nplt.ylim(0, 300)\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n```\n\n## Null Value Imputation\n\n```{python}\n# Step 1: Drop rows with missing bathrooms or bedrooms (small % of data)\n\ndf = pd.read_csv(\"airbnb.csv\") \n\ndf_clean = df.dropna(subset=[\"bathrooms\", \"bedrooms\"]).copy()\n\n# Step 2: Fill missing review score values with their respective medians\nreview_score_cols = [\"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_value\"]\nfor col in review_score_cols:\n    median_val = df_clean[col].median()\n    df_clean[col].fillna(median_val, inplace=True)\n\n# Step 3: Confirm no remaining missing values in relevant columns\nfinal_missing_check = df_clean.isnull().sum()\n\n# Display cleaned dataset shape and remaining missing values (should all be 0)\nprint(final_missing_check)\nprint(df_clean.shape)\nprint(df.shape)\n```\n\n## Model Building\n\n```{python}\n# One-hot encode room_type\n\ndf = pd.read_csv(\"airbnb.csv\") \n\ndf_clean = df.dropna(subset=[\"bathrooms\", \"bedrooms\"]).copy()\n\n# Step 2: Fill missing review score values with their respective medians\nreview_score_cols = [\"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_value\"]\nfor col in review_score_cols:\n    median_val = df_clean[col].median()\n    df_clean[col].fillna(median_val, inplace=True)\n\ndf_clean = pd.get_dummies(df_clean, columns=[\"room_type\"], drop_first=True)\n\n# Split into X and y\n# 1D â†’ 2D column vector\ny = df_clean[\"number_of_reviews\"].values  # âœ… 1D array\n\nprint('df_Clean columns',df_clean.columns)\n\n\nX = df_clean[[\"price\", \"days\", 'room_type_Private room', 'room_type_Shared room', \"bedrooms\", \"bathrooms\",\"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_value\"]]\n\nX = sm.add_constant(X)  # add intercept term\n\n# Convert all boolean columns to integers\n# Only cast if boolean columns exist\nbool_cols = X.select_dtypes('bool').columns\nif len(bool_cols) > 0:\n    X = X.astype({col: int for col in bool_cols})\n\n#print('Datatype of x',X.dtypes)\n#print('Null value check',df_clean.isnull().sum())\n# Fit OLS regression model\n\n# Now re-fit the model\nols_model = sm.OLS(y, X).fit()\n\n# Print summary\n\nprint(\"\\nðŸ“ˆ OLS Regression Summary:\")\nprint(ols_model.summary())\n```\n\n## âœ… Interpretation & Conclusion: OLS Regression on Airbnb Review Counts\n\n---\n\n### Interpretation of Key Coefficients\n\nEach coefficient represents the estimated change in the **number of reviews** (used as a proxy for bookings) given a one-unit change in the variable, **holding all else constant**.\n\n| Variable                   | Coefficient | Interpretation |\n|----------------------------|-------------|----------------|\n| **Intercept**              | 79.22       | Baseline number of reviews when all other features are zero (not directly interpretable, but part of the model). |\n| **Price**                  | -0.0022     | A $1 increase in price leads to a **small decrease** (~0.002) in the number of reviews. Suggests higher prices slightly reduce bookings. |\n| **Days Listed**            | 0.0021      | Each additional day the listing has been active adds ~0.002 more reviews. Bookings accumulate slowly over time. |\n| **Room Type: Private**     | -1.69       | Private rooms get ~1.7 fewer reviews than entire homes. |\n| **Room Type: Shared**      | -4.85       | Shared rooms get ~4.9 fewer reviews than entire homes â€” likely due to lower demand. |\n| **Bedrooms**               | 1.10        | Each additional bedroom increases expected reviews by ~1.1. Larger listings attract more guests. |\n| **Bathrooms**              | -1.74       | Surprisingly, each additional bathroom reduces expected reviews by ~1.74 â€” possibly because upscale listings have fewer but longer bookings. |\n| **Review Score: Cleanliness** | 0.85    | A 1-point increase in cleanliness score results in nearly 1 more review â€” cleanliness clearly matters to guests. |\n| **Review Score: Location**    | -4.05   | Unexpected: higher location score is associated with fewer reviews. This might reflect multicollinearity or other hidden variables. |\n| **Review Score: Value**       | -3.47   | Also unexpectedly negative â€” may reflect reverse causality: lower volume listings tend to receive high value ratings. |\n\n---\n\n### Conclusion\n\n- **Cleanliness**, **bedroom count**, and **room type** are **strong predictors** of Airbnb booking volume.\n- Listings with more **bedrooms** and better **cleanliness scores** receive more reviews.\n- **Private and shared rooms** consistently underperform compared to entire homes.\n- The negative relationship of **location** and **value scores** with review count suggests either model misspecification or deeper interactions worth exploring.\n\nâœ… Overall, this OLS model helps identify which property features are most closely associated with greater booking activity on Airbnb.\n","srcMarkdownNoYaml":"\n\n\n## Blueprinty Case Study\n\n### Introduction\n\nBlueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty's software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty's software and after using it. Unfortunately, such data is not available. \n\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm's number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty's software. The marketing team would like to use this data to make the claim that firms using Blueprinty's software are more successful in getting their patent applications approved.\n\ntodo: Read in data.\n\n### Data\n```{python}\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Read in the data\ndf = pd.read_csv(\"blueprinty.csv\")\n\n# Display basic info and preview\nprint(df.info())\nprint(df.head())\n```\n\ntodo: Compare histograms and means of number of patents by customer status. What do you observe?\n\n```{python}\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv(\"blueprinty.csv\")\n\n# Calculate mean number of patents by customer status\nmean_patents = df.groupby('iscustomer')['patents'].mean()\nprint(\"Mean number of patents:\\n\", mean_patents)\n\n# Plot histograms for customer vs non-customer\nplt.figure(figsize=(12, 6))\n\nplt.hist(df[df['iscustomer'] == 0]['patents'], bins=20, alpha=0.6, label='Non-Customers', color='red')\nplt.hist(df[df['iscustomer'] == 1]['patents'], bins=20, alpha=0.6, label='Customers', color='blue')\n\nplt.xlabel('Number of Patents')\nplt.ylabel('Number of Firms')\nplt.title('Patent Distribution by Blueprinty Usage')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n```\n### Histogram Analysis: Patent Distribution by Blueprinty Usage\n\n#### ðŸ”¹ Shift in Distribution\n- Customers have a **rightward shift** in their distribution compared to non-customers.\n- This suggests that **Blueprinty users tend to receive more patents**.\n\n#### ðŸ”¹ Higher Concentration at 4â€“8 Patents\n- The **blue bars dominate** in the range of **4 to 8 patents**, indicating a higher share of **high-performing firms** among Blueprinty users.\n\n#### ðŸ”¹ Non-Customers Clustered Lower\n- The **red bars are more concentrated** between **2 to 4 patents**, suggesting non-customers more frequently have **lower patent counts**.\n\n#### ðŸ”¹ Right-Tail Presence\n- A few firms with **10+ patents** appear in the distribution, primarily among Blueprinty users.\n- This may reflect a **subset of highly innovative firms** that benefit from using the software.\n\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\ntodo: Compare regions and ages by customer status. What do you observe?\n\n```{python}\n!pip install seaborn\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load the data\ndf = pd.read_csv(\"blueprinty.csv\")\n\n# Boxplot: Firm age by customer status\nplt.figure(figsize=(8, 5))\nsns.boxplot(x='iscustomer', y='age', data=df)\nplt.xticks([0, 1], ['Non-Customers', 'Customers'])\nplt.title('Firm Age by Blueprinty Customer Status')\nplt.xlabel('Blueprinty Customer')\nplt.ylabel('Firm Age')\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n# Cross-tabulation: Region by customer status (percentage within region)\nregion_counts = pd.crosstab(df['region'], df['iscustomer'], normalize='index') * 100\nregion_counts.columns = ['Non-Customers (%)', 'Customers (%)']\n\n# Sort by customer percentage and display\nregion_counts = region_counts.sort_values(by='Customers (%)', ascending=False)\nprint(\"\\nRegional Blueprinty Usage (%):\\n\")\nprint(region_counts.round(2))\n```\n### Systematic Differences in Customer vs. Non-Customer Firms\n\n1. **Age Distribution**:\n   - Blueprinty customers tend to be **slightly older** than non-customers, with a higher median firm age and more firms in the upper age range.\n\n2. **Regional Skew**:\n   - **Northeast** is the only region where a **majority of firms (54.6%) are customers**.\n   - All other regions (South, Southwest, Midwest, Northwest) have **customer rates below 20%**.\n\n3. **Customer Concentration**:\n   - Blueprinty adoption is **not uniform** across regions, with the **highest concentration of customers** in the **Northeast**.\n\n4. **Importance for Modeling**:\n   - Because age and region are **not randomly distributed** across customer groups, it's important to **control for both** when modeling patent outcomes to avoid biased estimates.\n\n\n### Estimation of Simple Poisson Model\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\n_todo: Write down mathematically the likelihood for_ $Y \\sim \\text{Poisson}(\\lambda)$. Note that $f(Y|\\lambda) = e^{-\\lambda}\\lambda^Y/Y!$.\n\n### Likelihood for Poisson Model\n\nWe model the number of patents \\($Y_i$\\) awarded to firm \\($i$\\) over 5 years as following a Poisson distribution:\n\n$$\nY_i \\sim \\text{Poisson}(\\lambda)\n$$\n\nThe probability mass function (PMF) for each observation is:\n\n$$\nf(Y_i | \\lambda) = \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n$$\n\nAssuming we have \\($n$\\) independent firms, the **joint likelihood function** is the product of all individual probabilities:\n\n$$\nL(\\lambda) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!} \n= e^{-n\\lambda} \\lambda^{\\sum_{i=1}^{n} Y_i} \\prod_{i=1}^{n} \\frac{1}{Y_i!}\n$$\n\n---\n\n### Log-Likelihood Function\n\nTo make the math easier for maximization, we take the logarithm of the likelihood function:\n\n$$\n\\ell(\\lambda) = \\log L(\\lambda) \n= -n\\lambda + \\left( \\sum_{i=1}^{n} Y_i \\right) \\log \\lambda - \\sum_{i=1}^{n} \\log(Y_i!)\n$$\n\nThis log-likelihood is what we will maximize to find the **Maximum Likelihood Estimate (MLE)** of ($\\lambda$\\)\n\n_todo: Code the likelihood (or log-likelihood) function for the Poisson model. This is a function of lambda and Y. For example:_\n\n```\npoisson_loglikelihood <- function(lambda, Y){\n   ...\n}\n```\n\n```{python}\nimport numpy as np\nfrom scipy.special import gammaln  # for log(Y!) using gammaln(Y+1)\n\ndef poisson_loglikelihood(lam, Y):\n    \"\"\"\n    Compute the log-likelihood of a Poisson model.\n    \n    Parameters:\n    - lam: float, the Poisson rate parameter Î»\n    - Y: array-like, observed count data (e.g., number of patents)\n    \n    Returns:\n    - log_likelihood: float, the total log-likelihood given Î» and Y\n    \"\"\"\n    Y = np.array(Y)\n    log_likelihood = -lam * len(Y) + np.sum(Y * np.log(lam) - gammaln(Y + 1))\n    return log_likelihood\n```\n\ntodo: Use your function to plot lambda on the horizontal axis and the likelihood (or log-likelihood) on the vertical axis for a range of lambdas (use the observed number of patents as the input for Y).\n\n```{python}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.special import gammaln\n\n# Load the dataset\ndf = pd.read_csv(\"blueprinty.csv\")\n\n# Define the Poisson log-likelihood function\ndef poisson_loglikelihood(lam, Y):\n    Y = np.array(Y)\n    return -lam * len(Y) + np.sum(Y * np.log(lam) - gammaln(Y + 1))\n\n# Use actual patent data from the dataset\nY_observed = df['patents'].values\n\n# Generate a range of lambda values to evaluate\nlambda_range = np.linspace(0.1, 10, 200)  # Avoid lambda = 0 to prevent log(0)\nlog_likelihoods = [poisson_loglikelihood(lam, Y_observed) for lam in lambda_range]\n\n# Plot the log-likelihood curve\nplt.figure(figsize=(10, 5))\nplt.plot(lambda_range, log_likelihoods, color='purple')\nplt.title(\"Poisson Log-Likelihood vs Lambda\")\nplt.xlabel(\"Lambda (Î»)\")\nplt.ylabel(\"Log-Likelihood\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n```\n\n\n```{python}\nimport pandas as pd\n\n# Load the dataset\ndf = pd.read_csv(\"blueprinty.csv\")\n\n# Calculate the MLE of lambda (mean of observed patent counts)\nlambda_mle = df['patents'].mean()\n\n# Print the result\nprint(f\"MLE of lambda (Î»Ì‚): {lambda_mle:.4f}\")\n```\n\n_todo: Find the MLE by optimizing your likelihood function with optim() in R or sp.optimize() in Python._  do it in python\n```{python}\nimport pandas as pd\nimport numpy as np\nfrom scipy.special import gammaln\nfrom scipy.optimize import minimize_scalar\n\n# Load the dataset\ndf = pd.read_csv(\"blueprinty.csv\")\nY_observed = df['patents'].values\n\n# Define the Poisson log-likelihood function\ndef poisson_loglikelihood(lam, Y):\n    Y = np.array(Y)\n    return -lam * len(Y) + np.sum(Y * np.log(lam) - gammaln(Y + 1))\n\n# Negative log-likelihood (since optimizers minimize by default)\ndef neg_poisson_loglikelihood(lam, Y):\n    return -poisson_loglikelihood(lam, Y)\n\n# Use scipy.optimize to find the lambda that minimizes the negative log-likelihood\nresult = minimize_scalar(neg_poisson_loglikelihood, bounds=(0.1, 10), args=(Y_observed,), method='bounded')\n\n# Extract the MLE of lambda\nlambda_mle = result.x\n\n# Print the result\nprint(f\"MLE of lambda (Î»Ì‚) from optimization: {lambda_mle:.4f}\")\n```\n\n\n### Estimation of Poisson Regression Model\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that $Y_i = \\text{Poisson}(\\lambda_i)$ where $\\lambda_i = \\exp(X_i'\\beta)$. The interpretation is that the success rate of patent awards is not constant across all firms ($\\lambda$) but rather is a function of firm characteristics $X_i$. Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n_todo: Update your likelihood or log-likelihood function with an additional argument to take in a covariate matrix X. Also change the parameter of the model from lambda to the beta vector. In this model, lambda must be a positive number, so we choose the inverse link function g_inv() to be exp() so that_ $\\lambda_i = e^{X_i'\\beta}$. _For example:_\n\npoisson_regression_likelihood <- function(beta, Y, X){\n   ...\n}\n\n```{python}\nimport numpy as np\nfrom scipy.special import gammaln        # stable log(y!)\n\ndef poisson_regression_loglik(beta, y, X):\n    \"\"\"\n    Log-likelihood for a Poisson GLM with log link.\n\n    Parameters\n    ----------\n    beta : array-like, shape (p,)\n        Coefficient vector (includes intercept if X has a 1s column).\n    y : array-like, shape (n,)\n        Observed non-negative counts.\n    X : array-like, shape (n, p)\n        Covariate matrix.\n\n    Returns\n    -------\n    float\n        â„“(Î²) = Î£ [ y_iÂ·(X_i Î²)  âˆ’  exp(X_i Î²)  âˆ’  log(y_i!) ].\n    \"\"\"\n    beta = np.asarray(beta, dtype=float)\n    y    = np.asarray(y,    dtype=float)\n\n    eta  = X @ beta            # linear predictor  (n,)\n    lam  = np.exp(eta)         # inverse-link â‡’ Î»_i > 0\n\n    return (y * eta  -  lam  -  gammaln(y + 1)).sum()\n```\n\n_todo: Use your function along with R's optim() or Python's sp.optimize() to find the MLE vector and the Hessian of the Poisson model with covariates. Specifically, the first column of X should be all 1's to enable a constant term in the model, and the subsequent columns should be age, age squared, binary variables for all but one of the regions, and the binary customer variable. Use the Hessian to find standard errors of the beta parameter estimates and present a table of coefficients and standard errors._\n\n```{python}\n!pip install scikit-learn\n!pip install scipy\nimport pandas as pd\nimport numpy as np\nfrom scipy.special import gammaln\nfrom scipy.optimize import minimize\nfrom numpy.linalg import inv\nfrom sklearn.preprocessing import StandardScaler\n\nimport pandas as pd\nimport statsmodels.api as sm          # convenient optimiser + Hessian\n\nblueprinty = pd.read_csv(\"blueprinty.csv\")\n\nX = pd.DataFrame({\n    \"const\"     : 1,                                     # intercept\n    \"age\"       : blueprinty[\"age\"],\n    \"age_sq\"    : blueprinty[\"age\"]**2,\n    \"region_NE\" : (blueprinty[\"region\"]==\"Northeast\").astype(int),\n    \"region_NW\" : (blueprinty[\"region\"]==\"Northwest\").astype(int),\n    \"region_S\"  : (blueprinty[\"region\"]==\"South\").astype(int),\n    \"region_SW\" : (blueprinty[\"region\"]==\"Southwest\").astype(int),\n    \"customer\"  : blueprinty[\"iscustomer\"]\n})\ny = blueprinty[\"patents\"]\n\n# â”€â”€ Poisson GLM (log link) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nmodel = sm.GLM(y, X, family=sm.families.Poisson())\nres   = model.fit()                      # uses IRLS â‡’ MLE, Hessian\n\nresults = pd.DataFrame({\n    \"Coefficient\" : res.params,\n    \"Std. Error\"  : res.bse\n})\nprint(\"Poisson Regression Results\", results.round(4))\n```\n\n_todo: Check your results using R's glm() function or Python sm.GLM() function._\n\n```{python}\nimport numpy as np, pandas as pd, statsmodels.api as sm\nfrom scipy.special import gammaln\nfrom scipy.optimize import minimize\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n\nXm = X.values\n\n# â”€â”€ Custom log-likelihood & optimiser â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ndef pll(beta, y, X):\n    eta = X @ beta\n    lam = np.exp(eta)\n    return (y*eta - lam - gammaln(y + 1)).sum()\n\ndef neg_pll(beta, y, X):\n    return -pll(beta, y, X)\n\nbeta0    = np.zeros(Xm.shape[1])\nopt_res  = minimize(neg_pll, beta0, args=(y, Xm), method=\"BFGS\")\nbeta_hat = opt_res.x                     # â‡  custom MLE vector\n\n# â”€â”€ Built-in GLM (IRLS) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nglm_res = sm.GLM(y, X, family=sm.families.Poisson()).fit()\n\n# â”€â”€ Side-by-side comparison â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ncompare = pd.DataFrame({\n    \"Custom Î²Ì‚\": beta_hat,\n    \"GLM Î²Ì‚\"   : glm_res.params,\n    \"|Î”|\"      : np.abs(beta_hat - glm_res.params)\n}, index=X.columns).round(6)\n\ndisplay(compare)\n```\n\n_todo: Interpret the results._ \n\n## ðŸ“Š Interpretation of Poisson Regression Results\n\nEach coefficient represents the **log change** in the expected number of patents for a **1-unit change** in the predictor, holding other variables constant. Since a Poisson model uses a **log link**, we interpret changes in **multiplicative (percentage) terms** using `exp(coef)`.\n\n---\n\n### ðŸ”¹ `const` (Intercept): -0.5089\n- This is the baseline log expected number of patents when all other variables are zero.\n- Not directly meaningful but needed for model completeness.\n\n---\n\n### ðŸ”¹ `age`: 0.1486  \n- A 1-year increase in firm age is associated with an **increase of exp(0.1486) â‰ˆ 1.16 times** more expected patents (~16% increase), holding all else constant.\n\n---\n\n### ðŸ”¹ `age_sq`: -0.0030  \n- Indicates a **non-linear effect** of age: as firms get older, the rate of patenting eventually slows down.\n- Suggests a concave (inverted U-shaped) relationship between age and patent output.\n\n---\n\n### ðŸ”¹ Region Dummies (`region_NE`, `region_NW`, `region_S`, `region_SW`)\n- All are compared to the **reference category (likely \"Midwest\")**.\n- Coefficients are small, implying **minor regional differences** in patenting rates.\n- For example:\n  - `region_S = 0.0566` â†’ firms in the South have ~6% higher expected patent counts than Midwest.\n\n---\n\n### ðŸ”¹ `customer`: 0.2076  \n- Being a Blueprinty customer increases the expected number of patents by **exp(0.2076) â‰ˆ 1.23**.\n- Thatâ€™s a **23% increase in patenting rate**, all else equal â€” which supports the marketing teamâ€™s claim.\n\n---\n\n### âœ… Summary Insight:\n- **Age has a significant positive effect**, but with diminishing returns.\n- **Using Blueprinty software is strongly associated with more patents**.\n- **Regional effects are minor**.\n\n\n_todo: What do you conclude about the effect of Blueprinty's software on patent success? Because the beta coefficients are not directly interpretable, it may help to create two fake datasets: X_0 and X_1 where X_0 is the X data but with iscustomer=0 for every observation and X_1 is the X data but with iscustomer=1 for every observation. Then, use X_0 and your fitted model to get the vector of predicted number of patents (y_pred_0) for every firm in the dataset, and use X_1 to get Y_pred_1 for every firm. Then subtract y_pred_1 minus y_pred_0 and take the average of that vector of differences._\n\n\n## Quantifying Blueprintyâ€™s Impact via Counterfactual Prediction\n\nGoal:\nTo understand the impact of Blueprinty's software on patent success,\nwe use a fitted Poisson regression model to simulate two scenarios:\n1. All firms are assumed to NOT use Blueprinty (iscustomer = 0)\n2. All firms are assumed to use Blueprinty (iscustomer = 1)\n\nWe then:\n- Predict patent counts for each firm under both scenarios\n- Calculate the average increase in patent count (y_pred_1 - y_pred_0)\n- Calculate the relative percentage increase due to Blueprinty\n\n```{python}\n# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nfrom scipy.special import gammaln\n\n# Load the dataset\ndf = pd.read_csv(\"blueprinty.csv\")\n\n# Create the design matrix X and response variable y\nX = pd.DataFrame({\n    \"const\"     : 1,\n    \"age\"       : df[\"age\"],\n    \"age_sq\"    : df[\"age\"]**2,\n    \"region_NE\" : (df[\"region\"] == \"Northeast\").astype(int),\n    \"region_NW\" : (df[\"region\"] == \"Northwest\").astype(int),\n    \"region_S\"  : (df[\"region\"] == \"South\").astype(int),\n    \"region_SW\" : (df[\"region\"] == \"Southwest\").astype(int),\n    \"customer\"  : df[\"iscustomer\"]\n})\ny = df[\"patents\"]\n\n# Fit Poisson GLM\nglm_model = sm.GLM(y, X, family=sm.families.Poisson()).fit()\n\n# ---- Counterfactual Predictions ----\n\n# Scenario 1: All firms are non-customers (iscustomer = 0)\nX_0 = X.copy()\nX_0[\"customer\"] = 0\ny_pred_0 = glm_model.predict(X_0)\n\n# Scenario 2: All firms are customers (iscustomer = 1)\nX_1 = X.copy()\nX_1[\"customer\"] = 1\ny_pred_1 = glm_model.predict(X_1)\n\n# Compute average treatment effect and percent lift\navg_diff = (y_pred_1 - y_pred_0).mean()\npct_increase = avg_diff / y_pred_0.mean()\n\n# ---- Output Results ----\nprint(\"ðŸ“Š Counterfactual Analysis of Blueprinty Impact\")\nprint(f\"Average increase in patents per firm: {avg_diff:.3f}\")\nprint(f\"Relative lift from Blueprinty usage: {pct_increase:.1%}\")\n```\n\n## âœ… Interpretation & Conclusion: Blueprinty's Impact on Patent Success\n\n### Interpretation\n\n- When assuming all firms are **non-customers** (`iscustomer = 0`), we predict their expected number of patents using the Poisson model.\n- When assuming all firms are **Blueprinty customers** (`iscustomer = 1`), the predicted patent counts increase.\n- The **average increase** in expected patent counts is: 0.793 additional patents per firm over 5 years\n- This translates to a **relative lift** of: 23.1% increase in patent output.\n\n\n- This effect holds after **controlling for other factors** such as firm age and regional location.\n\n---\n\n### Conclusion\n\n- Blueprinty's software is associated with a **significant and positive effect** on patent productivity.\n- On average, firms using Blueprinty are expected to receive **nearly one extra patent** over five years.\n- The **23.1% lift** is substantial, providing strong evidence to support Blueprintyâ€™s value proposition.\n- These results are statistically credible and align with intuitive expectations, reinforcing the case for adopting Blueprinty among engineering firms.\n\n## AirBnB Case Study\n\n### Introduction\n\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City.  The data include the following variables:\n\n:::: {.callout-note collapse=\"true\"}\n### Variable Definitions\n\n    - `id` = unique ID number for each unit\n    - `last_scraped` = date when information scraped\n    - `host_since` = date when host first listed the unit on Airbnb\n    - `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n    - `room_type` = Entire home/apt., Private room, or Shared room\n    - `bathrooms` = number of bathrooms\n    - `bedrooms` = number of bedrooms\n    - `price` = price per night (dollars)\n    - `number_of_reviews` = number of reviews for the unit on Airbnb\n    - `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n    - `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n    - `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n    - `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n::::\n\n\n_todo: Assume the number of reviews is a good proxy for the number of bookings. Perform some exploratory data analysis to get a feel for the data, handle or drop observations with missing values on relevant variables, build one or more models (e.g., a poisson regression model for the number of bookings as proxied by the number of reviews), and interpret model coefficients to describe variation in the number of reviews as a function of the variables provided._\n\n```{python}\n# ðŸ“Š Full Exploratory Data Analysis for Airbnb NYC Listings\n\n# Step 1: Import necessary libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\n\n# Step 2: Load the dataset\ndf = pd.read_csv(\"airbnb.csv\")  # Adjust path if needed\n\n# Step 3: Keep relevant columns\ncolumns_to_use = [\n    \"number_of_reviews\", \"room_type\", \"bathrooms\", \"bedrooms\", \"price\",\n    \"review_scores_cleanliness\", \"review_scores_location\",\n    \"review_scores_value\", \"instant_bookable\"\n]\ndf = df[columns_to_use].copy()\n\n# Step 5: Convert types\ndf[\"instant_bookable\"] = (df[\"instant_bookable\"] == \"t\").astype(int)\nnumeric_cols = [\n    \"number_of_reviews\", \"bathrooms\", \"bedrooms\", \"price\",\n    \"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_value\"\n]\ndf[numeric_cols] = df[numeric_cols].apply(pd.to_numeric, errors='coerce')\n\n# Step 6: Summary Statistics\nsummary_stats = df.describe()\nprint(\"ðŸ“‹ Summary Statistics:\")\nprint(summary_stats.round(2))\n\n# Step 7: Histogram of number_of_reviews\nplt.figure(figsize=(10, 5))\nsns.histplot(df[\"number_of_reviews\"], bins=50, kde=False)\nplt.title(\"Distribution of Number of Reviews\")\nplt.xlabel(\"Number of Reviews\")\nplt.ylabel(\"Count\")\nplt.xlim(0, 200)\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n# Step 8: Boxplot - Number of Reviews by Room Type\nplt.figure(figsize=(8, 5))\nsns.boxplot(data=df, x=\"room_type\", y=\"number_of_reviews\")\nplt.title(\"Number of Reviews by Room Type\")\nplt.xlabel(\"Room Type\")\nplt.ylabel(\"Number of Reviews\")\nplt.ylim(0, 200)\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n# Step 9: Correlation Heatmap\nplt.figure(figsize=(10, 8))\ncorr = df[numeric_cols + [\"instant_bookable\"]].corr()\nsns.heatmap(corr, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\nplt.title(\"Correlation Heatmap of Numeric Features\")\nplt.tight_layout()\nplt.show()\n\n# Step 10: Scatterplot - Price vs Number of Reviews\nplt.figure(figsize=(10, 5))\nsns.scatterplot(data=df, x=\"price\", y=\"number_of_reviews\", alpha=0.5)\nplt.title(\"Price vs. Number of Reviews\")\nplt.xlabel(\"Price per Night ($)\")\nplt.ylabel(\"Number of Reviews\")\nplt.xlim(0, 1000)\nplt.ylim(0, 300)\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n```\n\n## Null Value Imputation\n\n```{python}\n# Step 1: Drop rows with missing bathrooms or bedrooms (small % of data)\n\ndf = pd.read_csv(\"airbnb.csv\") \n\ndf_clean = df.dropna(subset=[\"bathrooms\", \"bedrooms\"]).copy()\n\n# Step 2: Fill missing review score values with their respective medians\nreview_score_cols = [\"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_value\"]\nfor col in review_score_cols:\n    median_val = df_clean[col].median()\n    df_clean[col].fillna(median_val, inplace=True)\n\n# Step 3: Confirm no remaining missing values in relevant columns\nfinal_missing_check = df_clean.isnull().sum()\n\n# Display cleaned dataset shape and remaining missing values (should all be 0)\nprint(final_missing_check)\nprint(df_clean.shape)\nprint(df.shape)\n```\n\n## Model Building\n\n```{python}\n# One-hot encode room_type\n\ndf = pd.read_csv(\"airbnb.csv\") \n\ndf_clean = df.dropna(subset=[\"bathrooms\", \"bedrooms\"]).copy()\n\n# Step 2: Fill missing review score values with their respective medians\nreview_score_cols = [\"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_value\"]\nfor col in review_score_cols:\n    median_val = df_clean[col].median()\n    df_clean[col].fillna(median_val, inplace=True)\n\ndf_clean = pd.get_dummies(df_clean, columns=[\"room_type\"], drop_first=True)\n\n# Split into X and y\n# 1D â†’ 2D column vector\ny = df_clean[\"number_of_reviews\"].values  # âœ… 1D array\n\nprint('df_Clean columns',df_clean.columns)\n\n\nX = df_clean[[\"price\", \"days\", 'room_type_Private room', 'room_type_Shared room', \"bedrooms\", \"bathrooms\",\"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_value\"]]\n\nX = sm.add_constant(X)  # add intercept term\n\n# Convert all boolean columns to integers\n# Only cast if boolean columns exist\nbool_cols = X.select_dtypes('bool').columns\nif len(bool_cols) > 0:\n    X = X.astype({col: int for col in bool_cols})\n\n#print('Datatype of x',X.dtypes)\n#print('Null value check',df_clean.isnull().sum())\n# Fit OLS regression model\n\n# Now re-fit the model\nols_model = sm.OLS(y, X).fit()\n\n# Print summary\n\nprint(\"\\nðŸ“ˆ OLS Regression Summary:\")\nprint(ols_model.summary())\n```\n\n## âœ… Interpretation & Conclusion: OLS Regression on Airbnb Review Counts\n\n---\n\n### Interpretation of Key Coefficients\n\nEach coefficient represents the estimated change in the **number of reviews** (used as a proxy for bookings) given a one-unit change in the variable, **holding all else constant**.\n\n| Variable                   | Coefficient | Interpretation |\n|----------------------------|-------------|----------------|\n| **Intercept**              | 79.22       | Baseline number of reviews when all other features are zero (not directly interpretable, but part of the model). |\n| **Price**                  | -0.0022     | A $1 increase in price leads to a **small decrease** (~0.002) in the number of reviews. Suggests higher prices slightly reduce bookings. |\n| **Days Listed**            | 0.0021      | Each additional day the listing has been active adds ~0.002 more reviews. Bookings accumulate slowly over time. |\n| **Room Type: Private**     | -1.69       | Private rooms get ~1.7 fewer reviews than entire homes. |\n| **Room Type: Shared**      | -4.85       | Shared rooms get ~4.9 fewer reviews than entire homes â€” likely due to lower demand. |\n| **Bedrooms**               | 1.10        | Each additional bedroom increases expected reviews by ~1.1. Larger listings attract more guests. |\n| **Bathrooms**              | -1.74       | Surprisingly, each additional bathroom reduces expected reviews by ~1.74 â€” possibly because upscale listings have fewer but longer bookings. |\n| **Review Score: Cleanliness** | 0.85    | A 1-point increase in cleanliness score results in nearly 1 more review â€” cleanliness clearly matters to guests. |\n| **Review Score: Location**    | -4.05   | Unexpected: higher location score is associated with fewer reviews. This might reflect multicollinearity or other hidden variables. |\n| **Review Score: Value**       | -3.47   | Also unexpectedly negative â€” may reflect reverse causality: lower volume listings tend to receive high value ratings. |\n\n---\n\n### Conclusion\n\n- **Cleanliness**, **bedroom count**, and **room type** are **strong predictors** of Airbnb booking volume.\n- Listings with more **bedrooms** and better **cleanliness scores** receive more reviews.\n- **Private and shared rooms** consistently underperform compared to entire homes.\n- The negative relationship of **location** and **value scores** with review count suggests either model misspecification or deeper interactions worth exploring.\n\nâœ… Overall, this OLS model helps identify which property features are most closely associated with greater booking activity on Airbnb.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"hw2_questions.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.43","extensions":["jmgirard/embedpdf"],"theme":["cosmo","brand"],"title":"Poisson Regression Examples","author":"Shruthi Suresh","date":"today","callout-appearance":"minimal"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}