{
  "hash": "7bed66b0da7416766ffbae4bbe160c5b",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Multinomial Logit Model\"\nauthor: \"Shruthi Suresh\"\ndate: today\n---\n\n\n\n\n\nThis assignment expores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm. \n\n\n## 1. Likelihood for the Multi-nomial Logit (MNL) Model\n\nSuppose we have $i=1,\\ldots,n$ consumers who each select exactly one product $j$ from a set of $J$ products. The outcome variable is the identity of the product chosen $y_i \\in \\{1, \\ldots, J\\}$ or equivalently a vector of $J-1$ zeros and $1$ one, where the $1$ indicates the selected product. For example, if the third product was chosen out of 3 products, then either $y=3$ or $y=(0,0,1)$ depending on how we want to represent it. Suppose also that we have a vector of data on each product $x_j$ (eg, brand, price, etc.). \n\nWe model the consumer's decision as the selection of the product that provides the most utility, and we'll specify the utility function as a linear function of the product characteristics:\n\n$$ U_{ij} = x_j'\\beta + \\epsilon_{ij} $$\n\nwhere $\\epsilon_{ij}$ is an i.i.d. extreme value error term. \n\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer $i$ chooses product $j$:\n\n$$ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} $$\n\nFor example, if there are 3 products, the probability that consumer $i$ chooses product 3 is:\n\n$$ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} $$\n\nA clever way to write the individual likelihood function for consumer $i$ is the product of the $J$ probabilities, each raised to the power of an indicator variable ($\\delta_{ij}$) that indicates the chosen product:\n\n$$ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}$$\n\nNotice that if the consumer selected product $j=3$, then $\\delta_{i3}=1$ while $\\delta_{i1}=\\delta_{i2}=0$ and the likelihood is:\n\n$$ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} $$\n\nThe joint likelihood (across all consumers) is the product of the $n$ individual likelihoods:\n\n$$ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} $$\n\nAnd the joint log-likelihood function is:\n\n$$ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) $$\n\n\n\n## 2. Simulate Conjoint Data\n\nWe will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a \"no choice\" option; each simulated respondent must select one of the 3 alternatives. \n\nEach alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from \\$4 to \\$32 in increments of \\$4.\n\nThe part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer $i$ for hypothethical streaming service $j$ is \n\n$$\nu_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n$$\n\nwhere the variables are binary indicators and $\\varepsilon$ is Type 1 Extreme Value (ie, Gumble) distributed.\n\nThe following code provides the simulation of the conjoint data.\n\n:::: {.callout-note collapse=\"true\"}\n\n::: {#13d1031c .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\nfrom itertools import product\n\n# Set seed for reproducibility\nnp.random.seed(123)\n\n# Define attributes\nbrands = ['N', 'P', 'H']  # Netflix, Prime, Hulu\nads = ['Yes', 'No']\nprices = np.arange(8, 33, 4)  # 8 to 32, step 4\n\n# Generate all possible profiles\nprofiles = pd.DataFrame(list(product(brands, ads, prices)), columns=[\"brand\", \"ad\", \"price\"])\nm = len(profiles)\n\n# Define part-worth utilities (true coefficients)\nb_util = {\"N\": 1.0, \"P\": 0.5, \"H\": 0.0}\na_util = {\"Yes\": -0.8, \"No\": 0.0}\np_util = lambda p: -0.1 * p\n\n# Simulation parameters\nn_peeps = 100\nn_tasks = 10\nn_alts = 3\n\n# Simulate one respondent’s data\ndef sim_one(id):\n    datalist = []\n    for t in range(1, n_tasks + 1):\n        sampled = profiles.sample(n=n_alts).copy()\n        sampled[\"resp\"] = id\n        sampled[\"task\"] = t\n        sampled[\"v\"] = sampled.apply(\n            lambda row: b_util[row[\"brand\"]] + a_util[row[\"ad\"]] + p_util(row[\"price\"]),\n            axis=1\n        )\n        sampled[\"e\"] = -np.log(-np.log(np.random.rand(n_alts)))  # Gumbel noise\n        sampled[\"u\"] = sampled[\"v\"] + sampled[\"e\"]\n        sampled[\"choice\"] = (sampled[\"u\"] == sampled[\"u\"].max()).astype(int)\n        datalist.append(sampled)\n    return pd.concat(datalist, ignore_index=True)\n\n# Simulate data for all respondents\nconjoint_data = pd.concat([sim_one(i) for i in range(1, n_peeps + 1)], ignore_index=True)\n\n# Keep only observable columns\nconjoint_data = conjoint_data[[\"resp\", \"task\", \"brand\", \"ad\", \"price\", \"choice\"]]\n\n# View sample\nconjoint_data.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>resp</th>\n      <th>task</th>\n      <th>brand</th>\n      <th>ad</th>\n      <th>price</th>\n      <th>choice</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>P</td>\n      <td>No</td>\n      <td>32</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n      <td>N</td>\n      <td>No</td>\n      <td>28</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>1</td>\n      <td>N</td>\n      <td>No</td>\n      <td>24</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>2</td>\n      <td>H</td>\n      <td>No</td>\n      <td>28</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>2</td>\n      <td>H</td>\n      <td>No</td>\n      <td>8</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::::\n\n\n\n## 3. Preparing the Data for Estimation\n\nThe \"hard part\" of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer $i$, covariate $k$, and product $j$) instead of the typical 2 dimensions for cross-sectional regression models (consumer $i$ and covariate $k$). The fact that each task for each respondent has the same number of alternatives (3) helps.  In addition, we need to convert the categorical variables for brand and ads into binary variables.\n\n::: {#e832ece9 .cell execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\n\n# Load the dataset\nconjoint_data = pd.read_csv(\"conjoint_data.csv\")\nprint(\"Columns in dataset:\", conjoint_data.columns.tolist())  # Confirm 'resp' and 'task' are present\n\n# Step 1: One-hot encode categorical variables (drop first level to avoid multicollinearity)\nX_df = pd.get_dummies(conjoint_data[[\"brand\", \"ad\"]], drop_first=True)\n\n# Step 2: Add numeric price\nX_df[\"price\"] = conjoint_data[\"price\"]\n\n# Step 3: Add the binary response variable\nX_df[\"choice\"] = conjoint_data[\"choice\"]\n\n# Step 4: Create design matrix and target\nX = X_df.drop(columns=[\"choice\"])\ny = X_df[\"choice\"]\n\n# Optional: Combine everything for easy preview\n# Convert X and y back to DataFrame with correct column names\nXy_df = pd.concat([X.reset_index(drop=True), y.reset_index(drop=True)], axis=1)\n\n# Add respondent and task info\nconjoint_ready = pd.concat(\n    [conjoint_data[[\"resp\", \"task\"]].reset_index(drop=True), Xy_df],\n    axis=1\n)\n\n# Step 5: Display the final prepared DataFrame\nprint(conjoint_ready.head())\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nColumns in dataset: ['resp', 'task', 'choice', 'brand', 'ad', 'price']\n   resp  task  brand_N  brand_P  ad_Yes  price  choice\n0     1     1     True    False    True     28       1\n1     1     1    False    False    True     16       0\n2     1     1    False     True    True     16       0\n3     1     2     True    False    True     32       0\n4     1     2    False     True    True     16       1\n```\n:::\n:::\n\n\n## 4. Estimation via Maximum Likelihood\n\n::: {#64481e5a .cell execution_count=3}\n``` {.python .cell-code}\n# -------------------------------------------\n# Step 0: Imports (run this after restarting kernel)\n# -------------------------------------------\nimport numpy as np\nimport pandas as pd\nfrom scipy.optimize import minimize\nfrom scipy.stats import norm\n\n# -------------------------------------------\n# Step 1: Extract Design Matrix X and Target y\n# -------------------------------------------\n# Assumes you already have `conjoint_ready` from previous steps\nX = conjoint_ready[[\"brand_N\", \"brand_P\", \"ad_Yes\", \"price\"]].values\ny = conjoint_ready[\"choice\"].values\nalt_per_task = 3  # 3 alternatives per choice task\n\n# Convert to float64 to ensure proper numerical operations\nX = X.astype(np.float64)\ny = y.astype(np.int32)\n\nprint(f\"X shape: {X.shape}\")\nprint(f\"y shape: {y.shape}\")\nprint(f\"X dtype: {X.dtype}\")\nprint(f\"y dtype: {y.dtype}\")\n\n# -------------------------------------------\n# Step 2: Define Log-Likelihood Function\n# -------------------------------------------\ndef mnl_log_likelihood(beta, X, y, alt_per_task=3):\n    \"\"\"\n    Compute the negative log-likelihood of the multinomial logit model.\n    \"\"\"\n    try:\n        n_obs = X.shape[0]\n        n_tasks = n_obs // alt_per_task\n        \n        # Ensure beta is float64\n        beta = np.array(beta, dtype=np.float64)\n        \n        # Calculate utilities\n        X_beta = np.dot(X, beta)\n        X_beta = X_beta.reshape((n_tasks, alt_per_task))\n        y_reshaped = y.reshape((n_tasks, alt_per_task))\n        \n        # Softmax with numerical stability\n        max_Xb = np.max(X_beta, axis=1, keepdims=True)\n        exp_terms = np.exp(X_beta - max_Xb)\n        log_sum_exp = np.log(np.sum(exp_terms, axis=1, keepdims=True))\n        log_probs = X_beta - max_Xb - log_sum_exp\n        \n        # Calculate negative log-likelihood\n        chosen_log_probs = log_probs[y_reshaped == 1]\n        neg_log_likelihood = -np.sum(chosen_log_probs)\n        \n        # Check for invalid values\n        if not np.isfinite(neg_log_likelihood):\n            return 1e10  # Return large value if optimization goes wrong\n            \n        return neg_log_likelihood\n        \n    except Exception as e:\n        print(f\"Error in log-likelihood function: {e}\")\n        return 1e10\n\n# -------------------------------------------\n# Step 3: Test the function first\n# -------------------------------------------\ninitial_beta = np.zeros(X.shape[1], dtype=np.float64)\nprint(f\"Initial beta: {initial_beta}\")\n\n# Test the log-likelihood function\ntest_ll = mnl_log_likelihood(initial_beta, X, y, alt_per_task)\nprint(f\"Initial log-likelihood: {test_ll}\")\n\n# -------------------------------------------\n# Step 4: Estimate Using scipy.optimize.minimize\n# -------------------------------------------\nprint(\"Starting optimization...\")\nresult = minimize(\n    mnl_log_likelihood, \n    initial_beta, \n    args=(X, y, alt_per_task), \n    method='BFGS',\n    options={'disp': True, 'maxiter': 1000}\n)\n\nprint(f\"Optimization successful: {result.success}\")\nprint(f\"Message: {result.message}\")\n\n# -------------------------------------------\n# Step 5: Extract Results and Standard Errors\n# -------------------------------------------\nif result.success:\n    beta_hat = result.x\n    \n    # Check if hessian inverse is available\n    if hasattr(result, 'hess_inv') and result.hess_inv is not None:\n        if isinstance(result.hess_inv, np.ndarray):\n            hessian_inv = result.hess_inv\n        else:\n            # Sometimes hess_inv is a LinearOperator, convert to array\n            hessian_inv = result.hess_inv.todense() if hasattr(result.hess_inv, 'todense') else np.array(result.hess_inv)\n        \n        std_errors = np.sqrt(np.diag(hessian_inv))\n        \n        # -------------------------------------------\n        # Step 6: Confidence Intervals (95%)\n        # -------------------------------------------\n        z = norm.ppf(0.975)\n        ci_lower = beta_hat - z * std_errors\n        ci_upper = beta_hat + z * std_errors\n        \n        # -------------------------------------------\n        # Step 7: Tabulate Results\n        # -------------------------------------------\n        param_names = [\"beta_netflix\", \"beta_prime\", \"beta_ads\", \"beta_price\"]\n        mle_results = pd.DataFrame({\n            \"Parameter\": param_names,\n            \"Estimate\": beta_hat,\n            \"Std_Error\": std_errors,\n            \"95% CI Lower\": ci_lower,\n            \"95% CI Upper\": ci_upper\n        })\n\n        # Save MLE results to CSV\n        mle_results.to_csv(\"mle_results.csv\", index=False)\n\n        \n        print(\"\\n\" + \"=\"*50)\n        print(\"MAXIMUM LIKELIHOOD ESTIMATION RESULTS\")\n        print(\"=\"*50)\n        print(mle_results.round(4))\n        \n    else:\n        print(\"Warning: Hessian inverse not available. Results without standard errors:\")\n        param_names = [\"beta_netflix\", \"beta_prime\", \"beta_ads\", \"beta_price\"]\n        basic_results = pd.DataFrame({\n            \"Parameter\": param_names,\n            \"Estimate\": beta_hat\n        })\n        print(basic_results.round(4))\n        \nelse:\n    print(\"Optimization failed!\")\n    print(f\"Message: {result.message}\")\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nX shape: (3000, 4)\ny shape: (3000,)\nX dtype: float64\ny dtype: int32\nInitial beta: [0. 0. 0. 0.]\nInitial log-likelihood: 1098.6122886681096\nStarting optimization...\nOptimization terminated successfully.\n         Current function value: 879.855368\n         Iterations: 13\n         Function evaluations: 95\n         Gradient evaluations: 19\nOptimization successful: True\nMessage: Optimization terminated successfully.\n\n==================================================\nMAXIMUM LIKELIHOOD ESTIMATION RESULTS\n==================================================\n      Parameter  Estimate  Std_Error  95% CI Lower  95% CI Upper\n0  beta_netflix    0.9412     0.1175        0.7109        1.1714\n1    beta_prime    0.5016     0.1210        0.2645        0.7387\n2      beta_ads   -0.7320     0.0889       -0.9062       -0.5578\n3    beta_price   -0.0995     0.0063       -0.1119       -0.0870\n```\n:::\n:::\n\n\n## 5. Estimation via Bayesian Methods\n\n::: {#3a9c1e2a .cell execution_count=4}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# -------------------------------------------\n# Step 1: Define log-prior function\n# -------------------------------------------\ndef log_prior(beta):\n    # Priors: N(0, 5^2) for binary variables, N(0, 1^2) for price\n    binary_priors = -0.5 * (beta[:3] / 5)**2 - 3 * np.log(5 * np.sqrt(2 * np.pi))\n    price_prior = -0.5 * (beta[3] / 1)**2 - np.log(np.sqrt(2 * np.pi))\n    return binary_priors.sum() + price_prior\n\n# -------------------------------------------\n# Step 2: Define log-posterior (log-likelihood + log-prior)\n# -------------------------------------------\ndef log_posterior(beta, X, y, alt_per_task):\n    loglik = -mnl_log_likelihood(beta, X, y, alt_per_task)  # log-likelihood is negative\n    logpri = log_prior(beta)\n    return loglik + logpri\n\n\n# -------------------------------------------\n# Step 3: Metropolis-Hastings Sampler\n# -------------------------------------------\nn_iter = 11000\nburn_in = 1000\nn_params = X.shape[1]\n\nsamples = np.zeros((n_iter, n_params))\ncurrent_beta = np.zeros(n_params)\ncurrent_logpost = log_posterior(current_beta, X, y, alt_per_task)\n\n# Proposal SDs: binary params N(0, 0.05), price param N(0, 0.005)\nproposal_sds = np.array([0.05, 0.05, 0.05, 0.005])\n\naccept_count = 0\n\nnp.random.seed(42)\n\nfor i in range(n_iter):\n    proposal = current_beta + np.random.normal(0, proposal_sds)\n    proposal_logpost = log_posterior(proposal, X, y, alt_per_task)\n\n    log_accept_ratio = proposal_logpost - current_logpost\n\n    if np.log(np.random.rand()) < log_accept_ratio:\n        current_beta = proposal\n        current_logpost = proposal_logpost\n        accept_count += 1\n\n    samples[i, :] = current_beta\n\naccept_rate = accept_count / n_iter\nprint(f\"Acceptance rate: {accept_rate:.3f}\")\n\n# -------------------------------------------\n# Step 4: Posterior Summary (after burn-in)\n# -------------------------------------------\nposterior_samples = samples[burn_in:, :]\nparam_names = [\"beta_netflix\", \"beta_prime\", \"beta_ads\", \"beta_price\"]\n\nposterior_df = pd.DataFrame(posterior_samples, columns=param_names)\n\nposterior_summary = posterior_df.describe(percentiles=[0.025, 0.975]).T\nposterior_summary = posterior_summary[[\"mean\", \"std\", \"2.5%\", \"97.5%\"]]\nposterior_summary.columns = [\"Mean\", \"Std_Dev\", \"95% CI Lower\", \"95% CI Upper\"]\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"BAYESIAN POSTERIOR SUMMARY\")\nprint(\"=\"*50)\nprint(posterior_summary.round(4))\n\nimport seaborn as sns\n\nplt.figure(figsize=(12, 5))\n\n# Trace plot\nplt.subplot(1, 2, 1)\nplt.plot(posterior_df[\"beta_price\"])\nplt.title(\"Trace Plot: beta_price\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Value\")\n\n# Histogram\nplt.subplot(1, 2, 2)\nsns.histplot(posterior_df[\"beta_price\"], bins=30, kde=True)\nplt.title(\"Posterior Distribution: beta_price\")\nplt.xlabel(\"Value\")\n\nplt.tight_layout()\nplt.show()\n\n\n\nimport pandas as pd\nimport os\n\n# -------------------------------------------\n# Debug the CSV loading issue\n# -------------------------------------------\n\n#file_path = \"C:\\\\Users\\\\Shruthi Suresh\\\\shruthis_website\\\\Projects\\\\Homework3\\\\mle_results.csv\"\n\n#mle_results = pd.read_csv(file_path)\n#print(\"Comparison: MLE vs Bayesian Estimates\")\n#combined = mle_results.set_index(\"Parameter\").join(posterior_summary, on=\"Parameter\")\n#print(combined.round(4))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAcceptance rate: 0.567\n\n==================================================\nBAYESIAN POSTERIOR SUMMARY\n==================================================\n                Mean  Std_Dev  95% CI Lower  95% CI Upper\nbeta_netflix  0.9458   0.1127        0.7321        1.1707\nbeta_prime    0.5067   0.1136        0.2882        0.7350\nbeta_ads     -0.7326   0.0831       -0.8909       -0.5668\nbeta_price   -0.0998   0.0063       -0.1119       -0.0873\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](hw3_questions_files/figure-html/cell-5-output-2.png){}\n:::\n:::\n\n\n## 6. Discussion\n\n\n### Interpreting Parameter Estimates:\n\nIf we had not simulated the data and were working with real-world conjoint responses, we would interpret the estimated parameters as revealed preferences derived from observed consumer choices. The fact that the model provides statistically significant estimates (with narrow confidence or credible intervals) suggests that the attributes included—brand, presence of ads, and price—are meaningful drivers of choice behavior.\n\n- **$\\beta_\\text{Netflix} > \\beta_\\text{Prime}$**: This result implies that, on average, consumers prefer **Netflix** over **Amazon Prime**, all else being equal. Since **Hulu** is the reference category (omitted in the dummy encoding), this also suggests that **Netflix** is the most preferred brand among the three, followed by **Prime**, then **Hulu**.\n\n- **$\\beta_\\text{price} < 0$**: A negative coefficient on price is not only intuitive but essential for model validity. It indicates that, all else being equal, higher-priced options are **less likely** to be chosen. The magnitude tells us how **sensitive** consumers are to price changes.\n\n---\n\n### Simulating and Estimating a Multi-Level (Hierarchical) Model\n\nTo move from a simple fixed-effects model to a **multi-level** (random-parameter or hierarchical) model, the key change is to allow **individual-level variation** in preferences. This means we no longer assume a single set of $\\beta$ coefficients for the entire population, but instead allow each respondent to have their own set of $\\beta_i$ values drawn from a population distribution.\n\n#### Key changes to simulate hierarchical data:\n\n- Instead of applying one common set of part-worth utilities, draw  \n  $\\beta_i \\sim \\mathcal{N}(\\mu, \\Sigma)$  \n  for each respondent $i$.\n\n- These $\\beta_i$ vectors are then used to compute utilities and simulate choices, allowing heterogeneity in behavior.\n\n#### Key changes for estimation:\n\n- Use **Bayesian hierarchical modeling** (e.g., via MCMC with group-level priors), or **Mixed Logit** models with simulated maximum likelihood.\n- Estimate the **distribution of preferences** (e.g., mean and covariance of $\\beta$ across individuals) instead of just point estimates.\n- Tools like **Stan**, **PyMC**, or hierarchical models in **scikit-learn** or **statsmodels** can be used.\n\nThis hierarchical approach better captures the reality that not all consumers behave the same and allows us to tailor predictions or develop segment-specific strategies accordingly.\n\n",
    "supporting": [
      "hw3_questions_files\\figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}