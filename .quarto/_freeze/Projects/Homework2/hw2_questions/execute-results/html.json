{
  "hash": "a512a7458f38fe714bdb665dab5c326d",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Poisson Regression Examples\"\nauthor: \"Shruthi Suresh\"\ndate: today\ncallout-appearance: minimal # this hides the blue \"i\" icon on .callout-notes\n---\n\n\n\n## Blueprinty Case Study\n\n### Introduction\n\nBlueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty's software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty's software and after using it. Unfortunately, such data is not available. \n\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm's number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty's software. The marketing team would like to use this data to make the claim that firms using Blueprinty's software are more successful in getting their patent applications approved.\n\ntodo: Read in data.\n\n### Data\n\n::: {#d27365a0 .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Read in the data\ndf = pd.read_csv(\"blueprinty.csv\")\n\n# Display basic info and preview\nprint(df.info())\nprint(df.head())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1500 entries, 0 to 1499\nData columns (total 4 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   patents     1500 non-null   int64  \n 1   region      1500 non-null   object \n 2   age         1500 non-null   float64\n 3   iscustomer  1500 non-null   int64  \ndtypes: float64(1), int64(2), object(1)\nmemory usage: 47.0+ KB\nNone\n   patents     region   age  iscustomer\n0        0    Midwest  32.5           0\n1        3  Southwest  37.5           0\n2        4  Northwest  27.0           1\n3        3  Northeast  24.5           0\n4        3  Southwest  37.0           0\n```\n:::\n:::\n\n\ntodo: Compare histograms and means of number of patents by customer status. What do you observe?\n\n::: {#b662be16 .cell execution_count=2}\n``` {.python .cell-code}\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv(\"blueprinty.csv\")\n\n# Calculate mean number of patents by customer status\nmean_patents = df.groupby('iscustomer')['patents'].mean()\nprint(\"Mean number of patents:\\n\", mean_patents)\n\n# Plot histograms for customer vs non-customer\nplt.figure(figsize=(12, 6))\n\nplt.hist(df[df['iscustomer'] == 0]['patents'], bins=20, alpha=0.6, label='Non-Customers', color='red')\nplt.hist(df[df['iscustomer'] == 1]['patents'], bins=20, alpha=0.6, label='Customers', color='blue')\n\nplt.xlabel('Number of Patents')\nplt.ylabel('Number of Firms')\nplt.title('Patent Distribution by Blueprinty Usage')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMean number of patents:\n iscustomer\n0    3.473013\n1    4.133056\nName: patents, dtype: float64\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](hw2_questions_files/figure-html/cell-3-output-2.png){}\n:::\n:::\n\n\n### Histogram Analysis: Patent Distribution by Blueprinty Usage\n\n#### ðŸ”¹ Shift in Distribution\n- Customers have a **rightward shift** in their distribution compared to non-customers.\n- This suggests that **Blueprinty users tend to receive more patents**.\n\n#### ðŸ”¹ Higher Concentration at 4â€“8 Patents\n- The **blue bars dominate** in the range of **4 to 8 patents**, indicating a higher share of **high-performing firms** among Blueprinty users.\n\n#### ðŸ”¹ Non-Customers Clustered Lower\n- The **red bars are more concentrated** between **2 to 4 patents**, suggesting non-customers more frequently have **lower patent counts**.\n\n#### ðŸ”¹ Right-Tail Presence\n- A few firms with **10+ patents** appear in the distribution, primarily among Blueprinty users.\n- This may reflect a **subset of highly innovative firms** that benefit from using the software.\n\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\ntodo: Compare regions and ages by customer status. What do you observe?\n\n::: {#390ed996 .cell execution_count=3}\n``` {.python .cell-code}\n!pip install seaborn\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load the data\ndf = pd.read_csv(\"blueprinty.csv\")\n\n# Boxplot: Firm age by customer status\nplt.figure(figsize=(8, 5))\nsns.boxplot(x='iscustomer', y='age', data=df)\nplt.xticks([0, 1], ['Non-Customers', 'Customers'])\nplt.title('Firm Age by Blueprinty Customer Status')\nplt.xlabel('Blueprinty Customer')\nplt.ylabel('Firm Age')\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n# Cross-tabulation: Region by customer status (percentage within region)\nregion_counts = pd.crosstab(df['region'], df['iscustomer'], normalize='index') * 100\nregion_counts.columns = ['Non-Customers (%)', 'Customers (%)']\n\n# Sort by customer percentage and display\nregion_counts = region_counts.sort_values(by='Customers (%)', ascending=False)\nprint(\"\\nRegional Blueprinty Usage (%):\\n\")\nprint(region_counts.round(2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRequirement already satisfied: seaborn in c:\\users\\shruthi suresh\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.13.2)\nRequirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\shruthi suresh\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from seaborn) (2.2.5)\nRequirement already satisfied: pandas>=1.2 in c:\\users\\shruthi suresh\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from seaborn) (2.2.3)\nRequirement already satisfied: matplotlib!=3.6.1,>=3.4 in c:\\users\\shruthi suresh\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from seaborn) (3.10.1)\nRequirement already satisfied: contourpy>=1.0.1 in c:\\users\\shruthi suresh\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in c:\\users\\shruthi suresh\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in c:\\users\\shruthi suresh\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.57.0)\nRequirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\shruthi suresh\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in c:\\users\\shruthi suresh\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (25.0)\nRequirement already satisfied: pillow>=8 in c:\\users\\shruthi suresh\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.2.1)\nRequirement already satisfied: pyparsing>=2.3.1 in c:\\users\\shruthi suresh\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.3)\nRequirement already satisfied: python-dateutil>=2.7 in c:\\users\\shruthi suresh\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in c:\\users\\shruthi suresh\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in c:\\users\\shruthi suresh\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\nRequirement already satisfied: six>=1.5 in c:\\users\\shruthi suresh\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\n[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n[notice] To update, run: python.exe -m pip install --upgrade pip\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](hw2_questions_files/figure-html/cell-4-output-3.png){}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\nRegional Blueprinty Usage (%):\n\n           Non-Customers (%)  Customers (%)\nregion                                     \nNortheast              45.42          54.58\nSouth                  81.68          18.32\nSouthwest              82.49          17.51\nMidwest                83.48          16.52\nNorthwest              84.49          15.51\n```\n:::\n:::\n\n\n### Systematic Differences in Customer vs. Non-Customer Firms\n\n1. **Age Distribution**:\n   - Blueprinty customers tend to be **slightly older** than non-customers, with a higher median firm age and more firms in the upper age range.\n\n2. **Regional Skew**:\n   - **Northeast** is the only region where a **majority of firms (54.6%) are customers**.\n   - All other regions (South, Southwest, Midwest, Northwest) have **customer rates below 20%**.\n\n3. **Customer Concentration**:\n   - Blueprinty adoption is **not uniform** across regions, with the **highest concentration of customers** in the **Northeast**.\n\n4. **Importance for Modeling**:\n   - Because age and region are **not randomly distributed** across customer groups, it's important to **control for both** when modeling patent outcomes to avoid biased estimates.\n\n\n### Estimation of Simple Poisson Model\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\n_todo: Write down mathematically the likelihood for_ $Y \\sim \\text{Poisson}(\\lambda)$. Note that $f(Y|\\lambda) = e^{-\\lambda}\\lambda^Y/Y!$.\n\n### Likelihood for Poisson Model\n\nWe model the number of patents \\($Y_i$\\) awarded to firm \\($i$\\) over 5 years as following a Poisson distribution:\n\n$$\nY_i \\sim \\text{Poisson}(\\lambda)\n$$\n\nThe probability mass function (PMF) for each observation is:\n\n$$\nf(Y_i | \\lambda) = \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n$$\n\nAssuming we have \\($n$\\) independent firms, the **joint likelihood function** is the product of all individual probabilities:\n\n$$\nL(\\lambda) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!} \n= e^{-n\\lambda} \\lambda^{\\sum_{i=1}^{n} Y_i} \\prod_{i=1}^{n} \\frac{1}{Y_i!}\n$$\n\n---\n\n### Log-Likelihood Function\n\nTo make the math easier for maximization, we take the logarithm of the likelihood function:\n\n$$\n\\ell(\\lambda) = \\log L(\\lambda) \n= -n\\lambda + \\left( \\sum_{i=1}^{n} Y_i \\right) \\log \\lambda - \\sum_{i=1}^{n} \\log(Y_i!)\n$$\n\nThis log-likelihood is what we will maximize to find the **Maximum Likelihood Estimate (MLE)** of ($\\lambda$\\)\n\n_todo: Code the likelihood (or log-likelihood) function for the Poisson model. This is a function of lambda and Y. For example:_\n\n```\npoisson_loglikelihood <- function(lambda, Y){\n   ...\n}\n```\n\n::: {#f39d0245 .cell execution_count=4}\n``` {.python .cell-code}\nimport numpy as np\nfrom scipy.special import gammaln  # for log(Y!) using gammaln(Y+1)\n\ndef poisson_loglikelihood(lam, Y):\n    \"\"\"\n    Compute the log-likelihood of a Poisson model.\n    \n    Parameters:\n    - lam: float, the Poisson rate parameter Î»\n    - Y: array-like, observed count data (e.g., number of patents)\n    \n    Returns:\n    - log_likelihood: float, the total log-likelihood given Î» and Y\n    \"\"\"\n    Y = np.array(Y)\n    log_likelihood = -lam * len(Y) + np.sum(Y * np.log(lam) - gammaln(Y + 1))\n    return log_likelihood\n```\n:::\n\n\ntodo: Use your function to plot lambda on the horizontal axis and the likelihood (or log-likelihood) on the vertical axis for a range of lambdas (use the observed number of patents as the input for Y).\n\n::: {#c9a830d0 .cell execution_count=5}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.special import gammaln\n\n# Load the dataset\ndf = pd.read_csv(\"blueprinty.csv\")\n\n# Define the Poisson log-likelihood function\ndef poisson_loglikelihood(lam, Y):\n    Y = np.array(Y)\n    return -lam * len(Y) + np.sum(Y * np.log(lam) - gammaln(Y + 1))\n\n# Use actual patent data from the dataset\nY_observed = df['patents'].values\n\n# Generate a range of lambda values to evaluate\nlambda_range = np.linspace(0.1, 10, 200)  # Avoid lambda = 0 to prevent log(0)\nlog_likelihoods = [poisson_loglikelihood(lam, Y_observed) for lam in lambda_range]\n\n# Plot the log-likelihood curve\nplt.figure(figsize=(10, 5))\nplt.plot(lambda_range, log_likelihoods, color='purple')\nplt.title(\"Poisson Log-Likelihood vs Lambda\")\nplt.xlabel(\"Lambda (Î»)\")\nplt.ylabel(\"Log-Likelihood\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](hw2_questions_files/figure-html/cell-6-output-1.png){}\n:::\n:::\n\n\n::: {#995dcf06 .cell execution_count=6}\n``` {.python .cell-code}\nimport pandas as pd\n\n# Load the dataset\ndf = pd.read_csv(\"blueprinty.csv\")\n\n# Calculate the MLE of lambda (mean of observed patent counts)\nlambda_mle = df['patents'].mean()\n\n# Print the result\nprint(f\"MLE of lambda (Î»Ì‚): {lambda_mle:.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMLE of lambda (Î»Ì‚): 3.6847\n```\n:::\n:::\n\n\n_todo: Find the MLE by optimizing your likelihood function with optim() in R or sp.optimize() in Python._  do it in python\n\n::: {#11cad839 .cell execution_count=7}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nfrom scipy.special import gammaln\nfrom scipy.optimize import minimize_scalar\n\n# Load the dataset\ndf = pd.read_csv(\"blueprinty.csv\")\nY_observed = df['patents'].values\n\n# Define the Poisson log-likelihood function\ndef poisson_loglikelihood(lam, Y):\n    Y = np.array(Y)\n    return -lam * len(Y) + np.sum(Y * np.log(lam) - gammaln(Y + 1))\n\n# Negative log-likelihood (since optimizers minimize by default)\ndef neg_poisson_loglikelihood(lam, Y):\n    return -poisson_loglikelihood(lam, Y)\n\n# Use scipy.optimize to find the lambda that minimizes the negative log-likelihood\nresult = minimize_scalar(neg_poisson_loglikelihood, bounds=(0.1, 10), args=(Y_observed,), method='bounded')\n\n# Extract the MLE of lambda\nlambda_mle = result.x\n\n# Print the result\nprint(f\"MLE of lambda (Î»Ì‚) from optimization: {lambda_mle:.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMLE of lambda (Î»Ì‚) from optimization: 3.6847\n```\n:::\n:::\n\n\n### Estimation of Poisson Regression Model\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that $Y_i = \\text{Poisson}(\\lambda_i)$ where $\\lambda_i = \\exp(X_i'\\beta)$. The interpretation is that the success rate of patent awards is not constant across all firms ($\\lambda$) but rather is a function of firm characteristics $X_i$. Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n_todo: Update your likelihood or log-likelihood function with an additional argument to take in a covariate matrix X. Also change the parameter of the model from lambda to the beta vector. In this model, lambda must be a positive number, so we choose the inverse link function g_inv() to be exp() so that_ $\\lambda_i = e^{X_i'\\beta}$. _For example:_\n\npoisson_regression_likelihood <- function(beta, Y, X){\n   ...\n}\n\n::: {#6f788313 .cell execution_count=8}\n``` {.python .cell-code}\nimport numpy as np\nfrom scipy.special import gammaln        # stable log(y!)\n\ndef poisson_regression_loglik(beta, y, X):\n    \"\"\"\n    Log-likelihood for a Poisson GLM with log link.\n\n    Parameters\n    ----------\n    beta : array-like, shape (p,)\n        Coefficient vector (includes intercept if X has a 1s column).\n    y : array-like, shape (n,)\n        Observed non-negative counts.\n    X : array-like, shape (n, p)\n        Covariate matrix.\n\n    Returns\n    -------\n    float\n        â„“(Î²) = Î£ [ y_iÂ·(X_i Î²)  âˆ’  exp(X_i Î²)  âˆ’  log(y_i!) ].\n    \"\"\"\n    beta = np.asarray(beta, dtype=float)\n    y    = np.asarray(y,    dtype=float)\n\n    eta  = X @ beta            # linear predictor  (n,)\n    lam  = np.exp(eta)         # inverse-link â‡’ Î»_i > 0\n\n    return (y * eta  -  lam  -  gammaln(y + 1)).sum()\n```\n:::\n\n\n_todo: Use your function along with R's optim() or Python's sp.optimize() to find the MLE vector and the Hessian of the Poisson model with covariates. Specifically, the first column of X should be all 1's to enable a constant term in the model, and the subsequent columns should be age, age squared, binary variables for all but one of the regions, and the binary customer variable. Use the Hessian to find standard errors of the beta parameter estimates and present a table of coefficients and standard errors._\n\n::: {#68eb0837 .cell execution_count=9}\n``` {.python .cell-code}\n!pip install scikit-learn\n!pip install scipy\nimport pandas as pd\nimport numpy as np\nfrom scipy.special import gammaln\nfrom scipy.optimize import minimize\nfrom numpy.linalg import inv\nfrom sklearn.preprocessing import StandardScaler\n\nimport pandas as pd\nimport statsmodels.api as sm          # convenient optimiser + Hessian\n\nblueprinty = pd.read_csv(\"blueprinty.csv\")\n\nX = pd.DataFrame({\n    \"const\"     : 1,                                     # intercept\n    \"age\"       : blueprinty[\"age\"],\n    \"age_sq\"    : blueprinty[\"age\"]**2,\n    \"region_NE\" : (blueprinty[\"region\"]==\"Northeast\").astype(int),\n    \"region_NW\" : (blueprinty[\"region\"]==\"Northwest\").astype(int),\n    \"region_S\"  : (blueprinty[\"region\"]==\"South\").astype(int),\n    \"region_SW\" : (blueprinty[\"region\"]==\"Southwest\").astype(int),\n    \"customer\"  : blueprinty[\"iscustomer\"]\n})\ny = blueprinty[\"patents\"]\n\n# â”€â”€ Poisson GLM (log link) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nmodel = sm.GLM(y, X, family=sm.families.Poisson())\nres   = model.fit()                      # uses IRLS â‡’ MLE, Hessian\n\nresults = pd.DataFrame({\n    \"Coefficient\" : res.params,\n    \"Std. Error\"  : res.bse\n})\nprint(\"Poisson Regression Results\", results.round(4))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRequirement already satisfied: scikit-learn in c:\\users\\shruthi suresh\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.6.1)\nRequirement already satisfied: numpy>=1.19.5 in c:\\users\\shruthi suresh\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (2.2.5)\nRequirement already satisfied: scipy>=1.6.0 in c:\\users\\shruthi suresh\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (1.15.2)\nRequirement already satisfied: joblib>=1.2.0 in c:\\users\\shruthi suresh\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (1.5.0)\nRequirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\shruthi suresh\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (3.6.0)\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\n[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n[notice] To update, run: python.exe -m pip install --upgrade pip\n\n[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n[notice] To update, run: python.exe -m pip install --upgrade pip\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nRequirement already satisfied: scipy in c:\\users\\shruthi suresh\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.15.2)\nRequirement already satisfied: numpy<2.5,>=1.23.5 in c:\\users\\shruthi suresh\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scipy) (2.2.5)\nPoisson Regression Results            Coefficient  Std. Error\nconst          -0.5089      0.1832\nage             0.1486      0.0139\nage_sq         -0.0030      0.0003\nregion_NE       0.0292      0.0436\nregion_NW      -0.0176      0.0538\nregion_S        0.0566      0.0527\nregion_SW       0.0506      0.0472\ncustomer        0.2076      0.0309\n```\n:::\n:::\n\n\n_todo: Check your results using R's glm() function or Python sm.GLM() function._\n\n::: {#e927c18f .cell execution_count=10}\n``` {.python .cell-code}\nimport numpy as np, pandas as pd, statsmodels.api as sm\nfrom scipy.special import gammaln\nfrom scipy.optimize import minimize\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n\nXm = X.values\n\n# â”€â”€ Custom log-likelihood & optimiser â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ndef pll(beta, y, X):\n    eta = X @ beta\n    lam = np.exp(eta)\n    return (y*eta - lam - gammaln(y + 1)).sum()\n\ndef neg_pll(beta, y, X):\n    return -pll(beta, y, X)\n\nbeta0    = np.zeros(Xm.shape[1])\nopt_res  = minimize(neg_pll, beta0, args=(y, Xm), method=\"BFGS\")\nbeta_hat = opt_res.x                     # â‡  custom MLE vector\n\n# â”€â”€ Built-in GLM (IRLS) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nglm_res = sm.GLM(y, X, family=sm.families.Poisson()).fit()\n\n# â”€â”€ Side-by-side comparison â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ncompare = pd.DataFrame({\n    \"Custom Î²Ì‚\": beta_hat,\n    \"GLM Î²Ì‚\"   : glm_res.params,\n    \"|Î”|\"      : np.abs(beta_hat - glm_res.params)\n}, index=X.columns).round(6)\n\ndisplay(compare)\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Custom Î²Ì‚</th>\n      <th>GLM Î²Ì‚</th>\n      <th>|Î”|</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>const</th>\n      <td>0.0</td>\n      <td>-0.508920</td>\n      <td>0.508920</td>\n    </tr>\n    <tr>\n      <th>age</th>\n      <td>0.0</td>\n      <td>0.148619</td>\n      <td>0.148619</td>\n    </tr>\n    <tr>\n      <th>age_sq</th>\n      <td>0.0</td>\n      <td>-0.002970</td>\n      <td>0.002970</td>\n    </tr>\n    <tr>\n      <th>region_NE</th>\n      <td>0.0</td>\n      <td>0.029170</td>\n      <td>0.029170</td>\n    </tr>\n    <tr>\n      <th>region_NW</th>\n      <td>0.0</td>\n      <td>-0.017575</td>\n      <td>0.017575</td>\n    </tr>\n    <tr>\n      <th>region_S</th>\n      <td>0.0</td>\n      <td>0.056561</td>\n      <td>0.056561</td>\n    </tr>\n    <tr>\n      <th>region_SW</th>\n      <td>0.0</td>\n      <td>0.050576</td>\n      <td>0.050576</td>\n    </tr>\n    <tr>\n      <th>customer</th>\n      <td>0.0</td>\n      <td>0.207591</td>\n      <td>0.207591</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n_todo: Interpret the results._ \n\n## ðŸ“Š Interpretation of Poisson Regression Results\n\nEach coefficient represents the **log change** in the expected number of patents for a **1-unit change** in the predictor, holding other variables constant. Since a Poisson model uses a **log link**, we interpret changes in **multiplicative (percentage) terms** using `exp(coef)`.\n\n---\n\n### ðŸ”¹ `const` (Intercept): -0.5089\n- This is the baseline log expected number of patents when all other variables are zero.\n- Not directly meaningful but needed for model completeness.\n\n---\n\n### ðŸ”¹ `age`: 0.1486  \n- A 1-year increase in firm age is associated with an **increase of exp(0.1486) â‰ˆ 1.16 times** more expected patents (~16% increase), holding all else constant.\n\n---\n\n### ðŸ”¹ `age_sq`: -0.0030  \n- Indicates a **non-linear effect** of age: as firms get older, the rate of patenting eventually slows down.\n- Suggests a concave (inverted U-shaped) relationship between age and patent output.\n\n---\n\n### ðŸ”¹ Region Dummies (`region_NE`, `region_NW`, `region_S`, `region_SW`)\n- All are compared to the **reference category (likely \"Midwest\")**.\n- Coefficients are small, implying **minor regional differences** in patenting rates.\n- For example:\n  - `region_S = 0.0566` â†’ firms in the South have ~6% higher expected patent counts than Midwest.\n\n---\n\n### ðŸ”¹ `customer`: 0.2076  \n- Being a Blueprinty customer increases the expected number of patents by **exp(0.2076) â‰ˆ 1.23**.\n- Thatâ€™s a **23% increase in patenting rate**, all else equal â€” which supports the marketing teamâ€™s claim.\n\n---\n\n### âœ… Summary Insight:\n- **Age has a significant positive effect**, but with diminishing returns.\n- **Using Blueprinty software is strongly associated with more patents**.\n- **Regional effects are minor**.\n\n\n_todo: What do you conclude about the effect of Blueprinty's software on patent success? Because the beta coefficients are not directly interpretable, it may help to create two fake datasets: X_0 and X_1 where X_0 is the X data but with iscustomer=0 for every observation and X_1 is the X data but with iscustomer=1 for every observation. Then, use X_0 and your fitted model to get the vector of predicted number of patents (y_pred_0) for every firm in the dataset, and use X_1 to get Y_pred_1 for every firm. Then subtract y_pred_1 minus y_pred_0 and take the average of that vector of differences._\n\n\n## Quantifying Blueprintyâ€™s Impact via Counterfactual Prediction\n\nGoal:\nTo understand the impact of Blueprinty's software on patent success,\nwe use a fitted Poisson regression model to simulate two scenarios:\n1. All firms are assumed to NOT use Blueprinty (iscustomer = 0)\n2. All firms are assumed to use Blueprinty (iscustomer = 1)\n\nWe then:\n- Predict patent counts for each firm under both scenarios\n- Calculate the average increase in patent count (y_pred_1 - y_pred_0)\n- Calculate the relative percentage increase due to Blueprinty\n\n::: {#25f7fc71 .cell execution_count=11}\n``` {.python .cell-code}\n# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nfrom scipy.special import gammaln\n\n# Load the dataset\ndf = pd.read_csv(\"blueprinty.csv\")\n\n# Create the design matrix X and response variable y\nX = pd.DataFrame({\n    \"const\"     : 1,\n    \"age\"       : df[\"age\"],\n    \"age_sq\"    : df[\"age\"]**2,\n    \"region_NE\" : (df[\"region\"] == \"Northeast\").astype(int),\n    \"region_NW\" : (df[\"region\"] == \"Northwest\").astype(int),\n    \"region_S\"  : (df[\"region\"] == \"South\").astype(int),\n    \"region_SW\" : (df[\"region\"] == \"Southwest\").astype(int),\n    \"customer\"  : df[\"iscustomer\"]\n})\ny = df[\"patents\"]\n\n# Fit Poisson GLM\nglm_model = sm.GLM(y, X, family=sm.families.Poisson()).fit()\n\n# ---- Counterfactual Predictions ----\n\n# Scenario 1: All firms are non-customers (iscustomer = 0)\nX_0 = X.copy()\nX_0[\"customer\"] = 0\ny_pred_0 = glm_model.predict(X_0)\n\n# Scenario 2: All firms are customers (iscustomer = 1)\nX_1 = X.copy()\nX_1[\"customer\"] = 1\ny_pred_1 = glm_model.predict(X_1)\n\n# Compute average treatment effect and percent lift\navg_diff = (y_pred_1 - y_pred_0).mean()\npct_increase = avg_diff / y_pred_0.mean()\n\n# ---- Output Results ----\nprint(\"ðŸ“Š Counterfactual Analysis of Blueprinty Impact\")\nprint(f\"Average increase in patents per firm: {avg_diff:.3f}\")\nprint(f\"Relative lift from Blueprinty usage: {pct_increase:.1%}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nðŸ“Š Counterfactual Analysis of Blueprinty Impact\nAverage increase in patents per firm: 0.793\nRelative lift from Blueprinty usage: 23.1%\n```\n:::\n:::\n\n\n## âœ… Interpretation & Conclusion: Blueprinty's Impact on Patent Success\n\n### Interpretation\n\n- When assuming all firms are **non-customers** (`iscustomer = 0`), we predict their expected number of patents using the Poisson model.\n- When assuming all firms are **Blueprinty customers** (`iscustomer = 1`), the predicted patent counts increase.\n- The **average increase** in expected patent counts is: 0.793 additional patents per firm over 5 years\n- This translates to a **relative lift** of: 23.1% increase in patent output.\n\n\n- This effect holds after **controlling for other factors** such as firm age and regional location.\n\n---\n\n### Conclusion\n\n- Blueprinty's software is associated with a **significant and positive effect** on patent productivity.\n- On average, firms using Blueprinty are expected to receive **nearly one extra patent** over five years.\n- The **23.1% lift** is substantial, providing strong evidence to support Blueprintyâ€™s value proposition.\n- These results are statistically credible and align with intuitive expectations, reinforcing the case for adopting Blueprinty among engineering firms.\n\n## AirBnB Case Study\n\n### Introduction\n\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City.  The data include the following variables:\n\n:::: {.callout-note collapse=\"true\"}\n### Variable Definitions\n\n    - `id` = unique ID number for each unit\n    - `last_scraped` = date when information scraped\n    - `host_since` = date when host first listed the unit on Airbnb\n    - `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n    - `room_type` = Entire home/apt., Private room, or Shared room\n    - `bathrooms` = number of bathrooms\n    - `bedrooms` = number of bedrooms\n    - `price` = price per night (dollars)\n    - `number_of_reviews` = number of reviews for the unit on Airbnb\n    - `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n    - `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n    - `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n    - `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n::::\n\n\n_todo: Assume the number of reviews is a good proxy for the number of bookings. Perform some exploratory data analysis to get a feel for the data, handle or drop observations with missing values on relevant variables, build one or more models (e.g., a poisson regression model for the number of bookings as proxied by the number of reviews), and interpret model coefficients to describe variation in the number of reviews as a function of the variables provided._\n\n::: {#c8a4a539 .cell execution_count=12}\n``` {.python .cell-code}\n# ðŸ“Š Full Exploratory Data Analysis for Airbnb NYC Listings\n\n# Step 1: Import necessary libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\n\n# Step 2: Load the dataset\ndf = pd.read_csv(\"airbnb.csv\")  # Adjust path if needed\n\n# Step 3: Keep relevant columns\ncolumns_to_use = [\n    \"number_of_reviews\", \"room_type\", \"bathrooms\", \"bedrooms\", \"price\",\n    \"review_scores_cleanliness\", \"review_scores_location\",\n    \"review_scores_value\", \"instant_bookable\"\n]\ndf = df[columns_to_use].copy()\n\n# Step 5: Convert types\ndf[\"instant_bookable\"] = (df[\"instant_bookable\"] == \"t\").astype(int)\nnumeric_cols = [\n    \"number_of_reviews\", \"bathrooms\", \"bedrooms\", \"price\",\n    \"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_value\"\n]\ndf[numeric_cols] = df[numeric_cols].apply(pd.to_numeric, errors='coerce')\n\n# Step 6: Summary Statistics\nsummary_stats = df.describe()\nprint(\"ðŸ“‹ Summary Statistics:\")\nprint(summary_stats.round(2))\n\n# Step 7: Histogram of number_of_reviews\nplt.figure(figsize=(10, 5))\nsns.histplot(df[\"number_of_reviews\"], bins=50, kde=False)\nplt.title(\"Distribution of Number of Reviews\")\nplt.xlabel(\"Number of Reviews\")\nplt.ylabel(\"Count\")\nplt.xlim(0, 200)\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n# Step 8: Boxplot - Number of Reviews by Room Type\nplt.figure(figsize=(8, 5))\nsns.boxplot(data=df, x=\"room_type\", y=\"number_of_reviews\")\nplt.title(\"Number of Reviews by Room Type\")\nplt.xlabel(\"Room Type\")\nplt.ylabel(\"Number of Reviews\")\nplt.ylim(0, 200)\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n# Step 9: Correlation Heatmap\nplt.figure(figsize=(10, 8))\ncorr = df[numeric_cols + [\"instant_bookable\"]].corr()\nsns.heatmap(corr, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\nplt.title(\"Correlation Heatmap of Numeric Features\")\nplt.tight_layout()\nplt.show()\n\n# Step 10: Scatterplot - Price vs Number of Reviews\nplt.figure(figsize=(10, 5))\nsns.scatterplot(data=df, x=\"price\", y=\"number_of_reviews\", alpha=0.5)\nplt.title(\"Price vs. Number of Reviews\")\nplt.xlabel(\"Price per Night ($)\")\nplt.ylabel(\"Number of Reviews\")\nplt.xlim(0, 1000)\nplt.ylim(0, 300)\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nðŸ“‹ Summary Statistics:\n       number_of_reviews  bathrooms  bedrooms     price  \\\ncount           40628.00   40468.00  40552.00  40628.00   \nmean               15.90       1.12      1.15    144.76   \nstd                29.25       0.39      0.69    210.66   \nmin                 0.00       0.00      0.00     10.00   \n25%                 1.00       1.00      1.00     70.00   \n50%                 4.00       1.00      1.00    100.00   \n75%                17.00       1.00      1.00    170.00   \nmax               421.00       8.00     10.00  10000.00   \n\n       review_scores_cleanliness  review_scores_location  review_scores_value  \\\ncount                   30433.00                30374.00             30372.00   \nmean                        9.20                    9.41                 9.33   \nstd                         1.12                    0.84                 0.90   \nmin                         2.00                    2.00                 2.00   \n25%                         9.00                    9.00                 9.00   \n50%                        10.00                   10.00                10.00   \n75%                        10.00                   10.00                10.00   \nmax                        10.00                   10.00                10.00   \n\n       instant_bookable  \ncount          40628.00  \nmean               0.19  \nstd                0.40  \nmin                0.00  \n25%                0.00  \n50%                0.00  \n75%                0.00  \nmax                1.00  \n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](hw2_questions_files/figure-html/cell-13-output-2.png){}\n:::\n\n::: {.cell-output .cell-output-display}\n![](hw2_questions_files/figure-html/cell-13-output-3.png){}\n:::\n\n::: {.cell-output .cell-output-display}\n![](hw2_questions_files/figure-html/cell-13-output-4.png){}\n:::\n\n::: {.cell-output .cell-output-display}\n![](hw2_questions_files/figure-html/cell-13-output-5.png){}\n:::\n:::\n\n\n## Null Value Imputation\n\n::: {#d5d76c14 .cell execution_count=13}\n``` {.python .cell-code}\n# Step 1: Drop rows with missing bathrooms or bedrooms (small % of data)\n\ndf = pd.read_csv(\"airbnb.csv\") \n\ndf_clean = df.dropna(subset=[\"bathrooms\", \"bedrooms\"]).copy()\n\n# Step 2: Fill missing review score values with their respective medians\nreview_score_cols = [\"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_value\"]\nfor col in review_score_cols:\n    median_val = df_clean[col].median()\n    df_clean[col].fillna(median_val, inplace=True)\n\n# Step 3: Confirm no remaining missing values in relevant columns\nfinal_missing_check = df_clean.isnull().sum()\n\n# Display cleaned dataset shape and remaining missing values (should all be 0)\nprint(final_missing_check)\nprint(df_clean.shape)\nprint(df.shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUnnamed: 0                    0\nid                            0\ndays                          0\nlast_scraped                  0\nhost_since                   34\nroom_type                     0\nbathrooms                     0\nbedrooms                      0\nprice                         0\nnumber_of_reviews             0\nreview_scores_cleanliness     0\nreview_scores_location        0\nreview_scores_value           0\ninstant_bookable              0\ndtype: int64\n(40395, 14)\n(40628, 14)\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nC:\\Users\\Shruthi Suresh\\AppData\\Local\\Temp\\ipykernel_5220\\1085593953.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df_clean[col].fillna(median_val, inplace=True)\nC:\\Users\\Shruthi Suresh\\AppData\\Local\\Temp\\ipykernel_5220\\1085593953.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df_clean[col].fillna(median_val, inplace=True)\nC:\\Users\\Shruthi Suresh\\AppData\\Local\\Temp\\ipykernel_5220\\1085593953.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df_clean[col].fillna(median_val, inplace=True)\n```\n:::\n:::\n\n\n## Model Building\n\n::: {#2b12e4fa .cell execution_count=14}\n``` {.python .cell-code}\n# One-hot encode room_type\n\ndf = pd.read_csv(\"airbnb.csv\") \n\ndf_clean = df.dropna(subset=[\"bathrooms\", \"bedrooms\"]).copy()\n\n# Step 2: Fill missing review score values with their respective medians\nreview_score_cols = [\"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_value\"]\nfor col in review_score_cols:\n    median_val = df_clean[col].median()\n    df_clean[col].fillna(median_val, inplace=True)\n\ndf_clean = pd.get_dummies(df_clean, columns=[\"room_type\"], drop_first=True)\n\n# Split into X and y\n# 1D â†’ 2D column vector\ny = df_clean[\"number_of_reviews\"].values  # âœ… 1D array\n\nprint('df_Clean columns',df_clean.columns)\n\n\nX = df_clean[[\"price\", \"days\", 'room_type_Private room', 'room_type_Shared room', \"bedrooms\", \"bathrooms\",\"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_value\"]]\n\nX = sm.add_constant(X)  # add intercept term\n\n# Convert all boolean columns to integers\n# Only cast if boolean columns exist\nbool_cols = X.select_dtypes('bool').columns\nif len(bool_cols) > 0:\n    X = X.astype({col: int for col in bool_cols})\n\n#print('Datatype of x',X.dtypes)\n#print('Null value check',df_clean.isnull().sum())\n# Fit OLS regression model\n\n# Now re-fit the model\nols_model = sm.OLS(y, X).fit()\n\n# Print summary\n\nprint(\"\\nðŸ“ˆ OLS Regression Summary:\")\nprint(ols_model.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ndf_Clean columns Index(['Unnamed: 0', 'id', 'days', 'last_scraped', 'host_since', 'bathrooms',\n       'bedrooms', 'price', 'number_of_reviews', 'review_scores_cleanliness',\n       'review_scores_location', 'review_scores_value', 'instant_bookable',\n       'room_type_Private room', 'room_type_Shared room'],\n      dtype='object')\n\nðŸ“ˆ OLS Regression Summary:\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.040\nModel:                            OLS   Adj. R-squared:                  0.040\nMethod:                 Least Squares   F-statistic:                     186.5\nDate:                Wed, 07 May 2025   Prob (F-statistic):               0.00\nTime:                        22:27:27   Log-Likelihood:            -1.9271e+05\nNo. Observations:               40395   AIC:                         3.854e+05\nDf Residuals:                   40385   BIC:                         3.855e+05\nDf Model:                           9                                         \nCovariance Type:            nonrobust                                         \n=============================================================================================\n                                coef    std err          t      P>|t|      [0.025      0.975]\n---------------------------------------------------------------------------------------------\nconst                        79.2197      2.041     38.805      0.000      75.218      83.221\nprice                        -0.0022      0.001     -2.917      0.004      -0.004      -0.001\ndays                          0.0021      0.000     20.216      0.000       0.002       0.002\nroom_type_Private room       -1.6934      0.304     -5.568      0.000      -2.289      -1.097\nroom_type_Shared room        -4.8517      0.850     -5.708      0.000      -6.518      -3.186\nbedrooms                      1.1033      0.232      4.760      0.000       0.649       1.558\nbathrooms                    -1.7445      0.410     -4.254      0.000      -2.548      -0.941\nreview_scores_cleanliness     0.8496      0.184      4.619      0.000       0.489       1.210\nreview_scores_location       -4.0500      0.217    -18.689      0.000      -4.475      -3.625\nreview_scores_value          -3.4706      0.243    -14.268      0.000      -3.947      -2.994\n==============================================================================\nOmnibus:                    30651.598   Durbin-Watson:                   1.648\nProb(Omnibus):                  0.000   Jarque-Bera (JB):           684469.728\nSkew:                           3.513   Prob(JB):                         0.00\nKurtosis:                      21.902   Cond. No.                     2.53e+04\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 2.53e+04. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nC:\\Users\\Shruthi Suresh\\AppData\\Local\\Temp\\ipykernel_5220\\108964921.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df_clean[col].fillna(median_val, inplace=True)\nC:\\Users\\Shruthi Suresh\\AppData\\Local\\Temp\\ipykernel_5220\\108964921.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df_clean[col].fillna(median_val, inplace=True)\nC:\\Users\\Shruthi Suresh\\AppData\\Local\\Temp\\ipykernel_5220\\108964921.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df_clean[col].fillna(median_val, inplace=True)\n```\n:::\n:::\n\n\n## âœ… Interpretation & Conclusion: OLS Regression on Airbnb Review Counts\n\n---\n\n### Interpretation of Key Coefficients\n\nEach coefficient represents the estimated change in the **number of reviews** (used as a proxy for bookings) given a one-unit change in the variable, **holding all else constant**.\n\n| Variable                   | Coefficient | Interpretation |\n|----------------------------|-------------|----------------|\n| **Intercept**              | 79.22       | Baseline number of reviews when all other features are zero (not directly interpretable, but part of the model). |\n| **Price**                  | -0.0022     | A $1 increase in price leads to a **small decrease** (~0.002) in the number of reviews. Suggests higher prices slightly reduce bookings. |\n| **Days Listed**            | 0.0021      | Each additional day the listing has been active adds ~0.002 more reviews. Bookings accumulate slowly over time. |\n| **Room Type: Private**     | -1.69       | Private rooms get ~1.7 fewer reviews than entire homes. |\n| **Room Type: Shared**      | -4.85       | Shared rooms get ~4.9 fewer reviews than entire homes â€” likely due to lower demand. |\n| **Bedrooms**               | 1.10        | Each additional bedroom increases expected reviews by ~1.1. Larger listings attract more guests. |\n| **Bathrooms**              | -1.74       | Surprisingly, each additional bathroom reduces expected reviews by ~1.74 â€” possibly because upscale listings have fewer but longer bookings. |\n| **Review Score: Cleanliness** | 0.85    | A 1-point increase in cleanliness score results in nearly 1 more review â€” cleanliness clearly matters to guests. |\n| **Review Score: Location**    | -4.05   | Unexpected: higher location score is associated with fewer reviews. This might reflect multicollinearity or other hidden variables. |\n| **Review Score: Value**       | -3.47   | Also unexpectedly negative â€” may reflect reverse causality: lower volume listings tend to receive high value ratings. |\n\n---\n\n### Conclusion\n\n- **Cleanliness**, **bedroom count**, and **room type** are **strong predictors** of Airbnb booking volume.\n- Listings with more **bedrooms** and better **cleanliness scores** receive more reviews.\n- **Private and shared rooms** consistently underperform compared to entire homes.\n- The negative relationship of **location** and **value scores** with review count suggests either model misspecification or deeper interactions worth exploring.\n\nâœ… Overall, this OLS model helps identify which property features are most closely associated with greater booking activity on Airbnb.\n\n",
    "supporting": [
      "hw2_questions_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}